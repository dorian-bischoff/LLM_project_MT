{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import (\n",
    "    generate_sorted_affinity_index,\n",
    "    generate_translation_several_models,\n",
    "    load_model_benchmark,\n",
    "    translate_batched_OPT,\n",
    "    translate_batched_Llama3,\n",
    "    get_sorted_affinity_index_path,\n",
    "    get_closest_sentences,\n",
    "    eval_metrics,\n",
    "    make_parallel_plot,\n",
    "    make_bar_plot,\n",
    "    make_bar_plot_all_metrics\n",
    ")\n",
    "\n",
    "from utils.eval_params import num_beams, temperature, max_new_tokens, top_p\n",
    "#num_beams = 5\n",
    "#max_new_tokens = 512\n",
    "#top_p = 0.9\n",
    "#temperature = 0.6\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells to generate sorted sentence's index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"wnt23\"\n",
    "for direction in [\"en-de\", \"de-en\",\n",
    "                  \"en-cs\", \"cs-en\",\n",
    "                  \"en-is\", \"is-en\",\n",
    "                  \"en-zh\", \"zh-en\",\n",
    "                  \"en-ru\", \"ru-en\"]:\n",
    "    print(f\"Generating sorted indexes based on BERT embedding simmilarities for direction {direction} and dataset {dataset_name}\")\n",
    "    generate_sorted_affinity_index(direction, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"flores\"\n",
    "for direction in [\"en-de\", \"de-en\",\n",
    "                  \"en-cs\", \"cs-en\",\n",
    "                  \"en-is\", \"is-en\",\n",
    "                  \"en-zh\", \"zh-en\",\n",
    "                  \"en-ru\", \"ru-en\"]:\n",
    "    print(f\"Generating sorted indexes based on BERT embedding simmilarities for direction {direction} and dataset {dataset_name}\")\n",
    "    generate_sorted_affinity_index(direction, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tgt_rag_fn_CausalModel(number_examples, dataset_name):\n",
    "    def get_input_targets_rag_CausalModel(dataset, source_lang, target_lang):\n",
    "        direction = f\"{source_lang}-{target_lang}\"\n",
    "        sort_aff_idx_savepath = get_sorted_affinity_index_path(direction, dataset_name)\n",
    "        with open(sort_aff_idx_savepath, \"rb\") as f:\n",
    "            sorted_affinity_index = pickle.load(f)\n",
    "\n",
    "        language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "        source_lang_name = language_name[source_lang]\n",
    "        target_lang_name = language_name[target_lang]\n",
    "        # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "        sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "\n",
    "        inputs = []\n",
    "        ds = dataset[direction]\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            examples = get_closest_sentences(number_examples, i, ds, sorted_affinity_index)\n",
    "            inp = f\"Here are examples of translations from {source_lang_name} to {target_lang_name}:\"\n",
    "            for n in range(number_examples):\n",
    "                example_source, example_target = examples[n][source_lang], examples[n][target_lang]\n",
    "                inp += f\"[START]\\n{source_lang_name}: {example_source} \\n{target_lang_name}: {example_target}\\n[END]\"\n",
    "            inp += f\"\\n Using the examples, translate from {source_lang_name} to {target_lang_name}:\"\n",
    "            input_source = dataset[f\"{source_lang}-{target_lang}\"][i][source_lang]\n",
    "            inp += f\"[START]\\n{source_lang_name}: {input_source} \\n{target_lang_name}:\"\n",
    "            inputs.append(inp)\n",
    "\n",
    "        targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "        return sources, inputs, targets\n",
    "    return get_input_targets_rag_CausalModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful, the code runs for ALMA, but 16GB is not enough to use 7B models quantized in 8 bits with one example...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "dataset_name = \"wnt23\"\n",
    "\n",
    "model_names = [\"opt-instruct\"]\n",
    "model_sizes = [None]\n",
    "\n",
    "batch_size = 1\n",
    "reduce_size = 4\n",
    "\n",
    "number_examples = 1\n",
    "\n",
    "# Careful to use generate_translation_several_models and not generate_translation_several_datasets (several datasets are not supported by the rag input generation)\n",
    "generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                    load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                    get_input_targets_fn = get_input_tgt_rag_fn_CausalModel(number_examples, dataset_name),\n",
    "                                    tslt_fn = translate_batched_OPT,\n",
    "                                    translation_folder = f\"evaluationsRAG_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for Instruct models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tgt_rag_fn_Instruct(number_examples, dataset_name):\n",
    "    def get_input_targets_rag_Instruct(dataset, source_lang, target_lang):\n",
    "        direction = f\"{source_lang}-{target_lang}\"\n",
    "        sort_aff_idx_savepath = get_sorted_affinity_index_path(direction, dataset_name)\n",
    "        with open(sort_aff_idx_savepath, \"rb\") as f:\n",
    "            sorted_affinity_index = pickle.load(f)\n",
    "\n",
    "        language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "        source_lang_name = language_name[source_lang]\n",
    "        target_lang_name = language_name[target_lang]\n",
    "        # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "        sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "\n",
    "        inputs = []\n",
    "        ds = dataset[direction]\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            examples = get_closest_sentences(number_examples, i, ds, sorted_affinity_index)\n",
    "            inp = f\"Here are examples of translations from {source_lang_name} to {target_lang_name}:\"\n",
    "            for n in range(number_examples):\n",
    "                example_source, example_target = examples[n][source_lang], examples[n][target_lang]\n",
    "                inp += f\"\\n[EXAMPLE {n+1}]\\n{source_lang_name}: {example_source} \\n{target_lang_name}: {example_target}\"\n",
    "            inp += f\"\\n Using the examples, translate from {source_lang_name} to {target_lang_name}:\"\n",
    "            input_source = dataset[f\"{source_lang}-{target_lang}\"][i][source_lang]\n",
    "            inp += f\"[TASK]\\n{source_lang_name}: {input_source} \\n{target_lang_name}:\"\n",
    "            inputs.append([\n",
    "            {\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{inp}\"}])\n",
    "\n",
    "        targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "        return sources, inputs, targets\n",
    "    return get_input_targets_rag_Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "dataset_name = \"wnt23\"\n",
    "\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]\n",
    "\n",
    "batch_size = 1\n",
    "reduce_size = 4\n",
    "\n",
    "for number_examples in [1, 2]: # More than 2 is OOM\n",
    "    # Careful to use generate_translation_several_models and not generate_translation_several_datasets (several datasets are not supported by the rag input generation)\n",
    "    generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                        load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                        get_input_targets_fn = get_input_tgt_rag_fn_Instruct(number_examples, dataset_name),\n",
    "                                        tslt_fn = translate_batched_Llama3,\n",
    "                                        translation_folder = f\"evaluationsRAG_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_examples in [1, 2]:\n",
    "    with open(f\"./generated_translations/evaluationsRAG_{number_examples}examples/wnt23_llama3-3B_en-de_red-50.pkl\", \"rb\") as f:\n",
    "        translations = pickle.load(f)\n",
    "    print(\"First translation:\", translations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metric_names = [\"rouge\", \"bleu\", \"sacrebleu\", \"chrf\", \"comet\", \"meteor\", \"bertscore\"]\n",
    "\n",
    "dataset_names = [\"wnt23\"] # Careful to use one dataset at a time otherwise get_input_tgt_rag_fn_Instruct will not be consistent\n",
    "reduce_sizes = [50]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_examples in [1, 2]:\n",
    "    eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes,\n",
    "                get_input_targets_fn=get_input_tgt_rag_fn_Instruct(number_examples, dataset_names[0]),\n",
    "                translation_folder=f\"evaluationsRAG_{number_examples}examples\",\n",
    "                additionnal_name=f\"RAG_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comput BLEURT metric alone because it is not offloaded of the GPU, need to restart the kernel...\n",
    "metric_names = [\"bleurt\"]\n",
    "\n",
    "dataset_names = [\"wnt23\"] # Careful to use one dataset at a time otherwise get_input_tgt_rag_fn_Instruct will not be consistent \n",
    "reduce_sizes = [50]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_examples in [1, 2]:\n",
    "    eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes,\n",
    "                get_input_targets_fn=get_input_tgt_rag_fn_Instruct(number_examples, dataset_names[0]),\n",
    "                translation_folder=f\"evaluationsRAG_{number_examples}examples\",\n",
    "                additionnal_name=f\"RAG_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\"]\n",
    "reduce_sizes = [50]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\",\n",
    "               \"llama3\",\n",
    "               \"llama3\"]\n",
    "model_sizes = [\"3B\",\n",
    "               \"3B\",\n",
    "               \"3B\"]\n",
    "additionnal_names = [None,\n",
    "                     \"RAG_1examples\",\n",
    "                     \"RAG_2examples\"]\n",
    "\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    additionnal_names=additionnal_names,\n",
    "                    savepath = \"./results/evaluations_figures/RAG_eval_parrallel_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_name = \"wnt23\"\n",
    "reduce_size = 50\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\",\n",
    "               \"llama3\",\n",
    "               \"llama3\"]\n",
    "model_sizes = [\"3B\",\n",
    "               \"3B\",\n",
    "               \"3B\"]\n",
    "additionnal_names = [None,\n",
    "                     \"RAG_1examples\",\n",
    "                     \"RAG_2examples\"]\n",
    "\n",
    "make_bar_plot(directions,\n",
    "                model_names, model_sizes,\n",
    "                dataset_name, reduce_size,\n",
    "                metric_names,\n",
    "                additionnal_names=additionnal_names,\n",
    "                width=0.2,\n",
    "                cmap=\"rainbow\",\n",
    "                savepath = \"./results/evaluations_figures/barplot_all_models_RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar plot per direction\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_name = \"wnt23\"\n",
    "reduce_size = 50\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\",\n",
    "               \"llama3\",\n",
    "               \"llama3\",]\n",
    "model_sizes = [\"3B\",\n",
    "               \"3B\",\n",
    "               \"3B\",]\n",
    "additionnal_names = [None,\n",
    "                     \"RAG_1examples\",\n",
    "                     \"RAG_2examples\"]\n",
    "\n",
    "make_bar_plot_all_metrics(directions, model_names, model_sizes, dataset_name, reduce_size, metric_names, additionnal_names,\n",
    "                        savepath = \"./results/evaluations_figures/RAG_barplot/barplot_all_metrics_ICL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
