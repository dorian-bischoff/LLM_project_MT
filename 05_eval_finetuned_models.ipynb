{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-17 22:15:36.796620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 22:15:36.817614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742249736.841333    8057 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742249736.848985    8057 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742249736.867145    8057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742249736.867174    8057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742249736.867176    8057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742249736.867178    8057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 22:15:36.873877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from utils import (\n",
    "    generate_translation_different_directions,\n",
    "    eval_metrics,\n",
    "    make_parallel_plot,\n",
    "    make_bar_plot,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"cs-en\", \"de-en\", \"is-en\", \"ru-en\", \"zh-en\",\n",
    "              \"en-cs\", \"en-de\", \"en-is\", \"en-ru\", \"en-zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model finetuned_llama3-3B for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:40<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 549/549 [00:00<00:00, 37790.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 549; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:54<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 104377.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1000; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:45<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model finetuned_llama3-3B for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:40<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 1976/1976 [00:00<00:00, 93324.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1976; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:39<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model finetuned_llama3-3B for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:37<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 557/557 [00:00<00:00, 42697.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 557; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:54<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 171441.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1000; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:40<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 2074/2074 [00:00<00:00, 135222.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2074; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:36<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model finetuned_llama3-3B for dataset wnt23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 2074/2074 [00:00<00:00, 135529.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2074; reduced to 100 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:36<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions=directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"finetuned_llama3-3B\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:47<00:00,  6.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:48<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:12<00:00,  7.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:52<00:00,  6.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:43<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:31<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:30<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:32<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:30<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model finetuned_llama3-3B for dataset flores...\n",
      "Total number of samples: 1012; reduced to 200 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:30<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions=directions,\n",
    "                                          dataset_name=\"flores\",\n",
    "                                          model_name=\"finetuned_llama3-3B\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=8,\n",
    "                                          reduce_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"rouge\", \"bleu\", \"sacrebleu\", \"chrf\", \"comet\", \"meteor\", \"bertscore\"]\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"finetuned_llama3-3B\"]\n",
    "model_sizes = [None]\n",
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"bleurt\"]\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"finetuned_llama3-3B\"]\n",
    "model_sizes = [None]\n",
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\"]\n",
    "reduce_sizes = [100]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"finetuned_llama3-3B\"]\n",
    "model_sizes = [None]\n",
    "\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\"],\n",
    "                    savepath = \"./results/evaluations_figures/finetuned_llama-3B_eval_directions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_name = \"wnt23\"\n",
    "reduce_size = 100\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\",\n",
    "               \"bloomz\",\n",
    "               \"finetuned_llama3-3B\"]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"3B\",\n",
    "               \"7B\",\n",
    "               None]\n",
    "\n",
    "make_bar_plot(directions,\n",
    "                model_names, model_sizes,\n",
    "                dataset_name, reduce_size,\n",
    "                metric_names,\n",
    "                width=0.15,\n",
    "                cmap=\"rainbow\",\n",
    "                savepath = \"./results/evaluations_figures/barplot_llama-3B_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP environment",
   "language": "python",
   "name": "snlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
