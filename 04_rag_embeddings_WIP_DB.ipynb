{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorian/miniconda3/envs/altegrad-challenge/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert():\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "    model = transformers.BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict_bert(tokenizer, model, sentence):\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(sentence, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "    return output\n",
    "\n",
    "def get_bert_embedding(tokenizer, model, sentence):\n",
    "    out = predict_bert(tokenizer, model, sentence) # [1, ntokens, 768]\n",
    "    out = out[0][:,  1:-1, :] # Remove CLS and SEP tokens -> [1, ntokens-2, 768]\n",
    "    out = out.mean(dim=1) # [1, 768]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Use BERT to generate embeddings \n",
    "def get_sorted_affinity_index(list_sentence):\n",
    "    tokenizer, model = load_bert()\n",
    "    bert_embeddings = []\n",
    "    for sentence_to_translate in list_sentence:\n",
    "        bert_embeddings.append(get_bert_embedding(tokenizer, model, sentence_to_translate)) # [1, emb_size]\n",
    "    bert_embeddings = torch.cat(bert_embeddings, dim=0) # [n_sentences, emb_size]\n",
    "    affinity = bert_embeddings @ bert_embeddings.T # [n_sentences, n_sentences]\n",
    "    affinity -= torch.eye(affinity.shape, device = affinity.device)*float(\"Inf\") #  Suppress self-affinity\n",
    "\n",
    "    sorted_affinity_index = []\n",
    "    for i in range(len(list_sentence)):\n",
    "        sorted_idx = torch.argsort(affinity[i],  descending = True).tolist()\n",
    "        sorted_affinity_index.append(sorted_idx)\n",
    "    return sorted_affinity_index\n",
    "\n",
    "def get_closest_sentences(nb_sentences, sentence_idx, list_sentence, sorted_affinity_index):\n",
    "    closest_sentences = []\n",
    "    for i in sorted_affinity_index[sentence_idx][:nb_sentences]:\n",
    "        closest_sentences.append(list_sentence[i])\n",
    "    return closest_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(inputs: list[str], sources: list[str], targets, final_nb: list[str]) -> tuple[list[str], list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Selects randomly the samples of the evaluation corpus\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(inputs))\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(idx, final_nb)\n",
    "    return [inputs[i] for i in idx], [sources[i] for i in idx], [targets[i] for i in idx]\n",
    "\n",
    "def get_input_tgt_rag_fn(number_examples, sorted_affinity_index):\n",
    "    def get_input_targets_rag_ALMA(dataset, source_lang, target_lang):\n",
    "        language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "        source_lang_name = language_name[source_lang]\n",
    "        target_lang_name = language_name[target_lang]\n",
    "        # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "        sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "\n",
    "        inputs = []\n",
    "        for i in range(len(dataset)):\n",
    "            examples = get_closest_sentences(number_examples, i, sources, sorted_affinity_index)\n",
    "            inp = f\"Here are examples of translations from {source_lang_name} to {target_lang_name}:\"\n",
    "            for n in range(number_examples):\n",
    "                example_source, example_target = examples[n][source_lang], examples[n][target_lang]\n",
    "                inp += f\"\\n{source_lang_name}: {example_source} \\n{target_lang_name}: {example_target} \"\n",
    "            inp += f\"\\n Using the examples, translate from {source_lang_name} to {target_lang_name}:\"\n",
    "            input_source = dataset[f\"{source_lang}-{target_lang}\"][i][source_lang]\n",
    "            inp += f\"\\n{source_lang_name}: {input_source} \\n{target_lang_name}:\"\n",
    "            inputs.append(inp)\n",
    "\n",
    "        targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "        return sources, inputs, targets\n",
    "    return get_input_targets_rag_ALMA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
