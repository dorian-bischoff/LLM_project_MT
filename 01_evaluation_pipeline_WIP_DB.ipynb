{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 23:13:48.215409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740262428.229733   17960 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740262428.234038   17960 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-22 23:13:48.249030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import peft\n",
    "from datasets import Dataset\n",
    "import huggingface_hub\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tested = [\"en\", \"de\", \"cs\", \"is\", \"zh\", \"ru\"] # Only from or to english\n",
    "evaluated_metrics = [\"bleu\", \"rouge\", \"bleurt\", \"sacrebleu\", \"comet\", \"meteor\", \"chrf\", \"bert_score\"]\n",
    "models = [\n",
    "    \"facebook/nllb-200-distilled-600M\",\n",
    "    \"haoranxu/ALMA-7B\",\n",
    "\n",
    "\n",
    "\n",
    "    \"meta-llama/Meta-Llama-3-8B\", # TODO\n",
    "    \"mistralai/Mamba-Codestral-7B-v0.1\", # TODO\n",
    "    \"mt5-base\",\n",
    "    \"gpt2\", # TODO\n",
    "    \"Qwen/Qwen2-0.5B\", # TODO\n",
    "] # TODO falcon 7B, mistral 7B, Bloom 7B, OPT 7B, MPT 7B, Bayling (?)\n",
    "\n",
    "ds = [\"haoranxu/WMT23-Test\"] # TODO FLORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling\n",
    "We use WNT23 from the authors preprocessed split and the FLORES+ dataset, format in the same way that the WNT23 is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNT23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557 {'en-de': [{'en': 'Police arrest 15 after violent protest outside UK refugee hotel', 'de': 'Polizei verhaftet 15 Menschen nach gewalttätigen Protesten vor einer Flüchtlingsunterkunft in Großbritannien'}, {'en': 'The incident comes after increase in numbers of refugees and asylum seekers crossing the Channel to the UK in boats. Police have arrested 15 people after an anti-refugee demonstration outside a hotel used to house asylum seekers turned violent near the English city of Liverpool. The Merseyside Police department said a police officer and two civilians sustained minor injuries during the disturbance on Friday night in Knowsley. The police force said some protesters threw objects and set a police van on fire. The people arrested, who ranged in age from 13 to 54, were detained \"following violent disorder.\" Merseyside police commissioner Emily Spurrell told Radio City, \"It was incredibly dangerous and there were a couple of injuries amongst the police officers.\"', 'de': 'Der Vorfall ereignet sich, nachdem sich die Zahl der Flüchtlinge und Asylbewerber, die den Ärmelkanal nach Großbritannien in Booten überqueren, ansteigt. Die Polizei hat 15 Menschen verhaftet, nachdem Anti-Flüchtlingsdemonstrationen vor einem Hotel in der Nähe der englischen Stadt Liverpool, das verwendet wird, um Asylbewerber unterzubringen, gewalttätig wurden. Laut dem Polizeirevier von Merseyside erlitten ein Polizeibeamter und zwei Zivilisten während des Tumults am Freitagabend in Knowsley leichte Verletzungen. Die Polizei gab bekannt, dass einige Demonstranten Gegenstände warfen und einen Polizeiwagen anzündeten. Die verhafteten Menschen, die zwischen 13 und 54 Jahren alt waren, wurden „aufgrund von gewalttätigen Ausschreitungen“ inhaftiert. Die Polizeikommissarin von Merseyside Emily Spurrell äußerte gegenüber Radio City, „Es war unglaublich gefährlich und es gab einige Verletzte unter den Polizeibeamten.“'}, {'en': 'The Home Office has been using the hotel to temporarily house asylum seekers since last year, according to local media. George Howarth, who represents Knowsley in the UK Parliament, said the violence on Friday night did not reflect the community. \"The people of Knowsley are not bigots and are welcoming to people escaping from some of the most dangerous places in the world in search of a place of safety,\" he said. \"Those demonstrating against refugees at this protest tonight do not represent this community.\" The protest took place amid heightened tensions as growing numbers of refugees and migrants cross the Channel in small boats.', 'de': 'Das Innenministerium hat das Hotel seit letztem Jahr verwendet, um vorübergehend Asylbewerber unterzubringen, so die lokalen Medien. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend nicht die Gemeinde widerspiegelt. „Die Einwohner von Knowsley sind keine Rassisten und sind gastfreundlich gegenüber Menschen, die von einigen der gefährlichsten Orten der Welt auf der Suche nach einem sicheren Ort fliehen“, sagte er. „Diejenigen, die bei heute Abend bei diesem Protest demonstrieren, spiegeln diese Gemeinde nicht wider.“ Der Protest fand unter erhöhten Spannungen statt, während wachsende Zahlen von Flüchtlingen und Migranten den Ärmelkanal in kleinen Booten überqueren.'}, {'en': 'More than 45,000 people reached the UK by that route in 2022, and most applied for asylum. The system for considering asylum applications has slowed to a crawl because of political turmoil and bureaucratic delays, leaving many asylum seekers stuck in hotels or other temporary accommodations. The Channel crossings have become a political issue, with the Conservative government promising to \"stop the boats\" and pursuing a plan to send such asylum seekers to Rwanda. Opponents have accused the government of demonising desperate people fleeing war and poverty.', 'de': 'Mehr als 45.000 Menschen haben 2022 Großbritannien auf diesem Weg erreicht und die meisten haben einen Asylantrag gestellt. Das System, das Asylanträge bearbeitet, hat sich aufgrund politischer Unruhen und bürokratischer Verzögerungen stark verlangsamt, was dazu führte, dass viele Asylbewerber in Hotels und vorübergehenden Unterkünften feststecken. Die Überquerungen des Ärmelkanals sind zu einem politischen Thema geworden, die konservative Regierungen verspricht hierbei „die Boote zu stoppen“ und verfolgt einen Plan, jene Asylbewerber nach Ruanda zu schicken. Gegner haben der Regierung vorgeworfen, verzweifelte Menschen, die vor Krieg und Armut fliehen, zu dämonisieren.'}]}\n"
     ]
    }
   ],
   "source": [
    "ds_wnt = load_dataset(\"haoranxu/WMT23-Test\", \"en-de\")[\"test\"]\n",
    "print(len(ds_wnt), ds_wnt[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bedb4a6e38340b99c3c2bec9059af7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a04860df9a54201be2c0827705e8757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6dcd023a0f4705a1598b52b48f25c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0752df7916d54b5cbc79f7fc93f9531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_WNT_style(ds_flores, lang, lang_start=\"en\"):\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"zho\", \"ru\": \"rus\"}\n",
    "    list_sentence_lang, list_sentence_lang_start = [], []\n",
    "    for elem in ds_flores:\n",
    "        if elem['iso_639_3'] == language_to_iso[lang]:\n",
    "            list_sentence_lang.append(elem[\"text\"])\n",
    "        elif elem['iso_639_3'] == language_to_iso[lang_start]:\n",
    "            list_sentence_lang_start.append(elem[\"text\"])\n",
    "    assert len(list_sentence_lang) == len(list_sentence_lang_start)\n",
    "    print(f\"Number of samples: {len(list_sentence_lang)}\")\n",
    "    final_text_list = []\n",
    "    for i in range(len(list_sentence_lang)):\n",
    "        final_text_list.append({f\"{lang_start}\": list_sentence_lang_start[i],\n",
    "                                f\"{lang}\": list_sentence_lang[i],})\n",
    "    return Dataset.from_dict({f\"{lang_start}-{lang}\": final_text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1012\n",
      "{'en-de': [{'de': '„Wir haben jetzt 4 Monate alte Mäuse, die Diabetes hatten und jetzt keinen mehr haben“, fügte er hinzu.', 'en': '\"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.'}, {'de': 'Dr. Ehud Ur, Professor für Medizin an der Dalhousie University in Halifax, Nova Scotia, und Vorsitzender der Abteilung für Klinik und Wissenschaft des Kanadischen Diabetesverbands gab zu bedenken, dass die Forschungsarbeit noch in den Kinderschuhen stecke.', 'en': 'Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.'}, {'de': 'Wie einige andere Experten zeigte er sich skeptisch, ob es möglich sei, Diabetes zu heilen, und wies darauf hin, dass die Befunde für Menschen, die bereits unter Typ-1-Diabetes litten, keine Bedeutung hätten.', 'en': 'Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.'}, {'de': 'Am Montag gab Sara Danius, Ständige Sekretärin des Literaturnobelpreiskomitees der Schwedischen Akademie in einer Radiosendung auf Sveriges Radio in Schweden bekannt, dass das Komitee seine Bemühungen eingestellt habe, Bob Dylan über seine Ehrung mit dem Literaturnobelpreis für Literator zu informieren, nachdem es nicht gelungen war, ihn persönlich zu erreichen.', 'en': 'On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him.'}]}\n"
     ]
    }
   ],
   "source": [
    "ds_flores_wnt_style = transform_to_WNT_style(ds_flores, lang=\"de\", lang_start=\"en\")\n",
    "print(ds_flores_wnt_style[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from predictions: evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(sources, targets, translation_infered):\n",
    "    print(\"Computing BLEU...\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results_bleu = bleu.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing ROUGE...\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results_rouge = rouge.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing BLEURT...\")\n",
    "    bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "    results_bleurt = bleurt.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing SACREBLEU...\")\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "    results_sacrebleu = sacrebleu.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing COMET...\")\n",
    "    comet_metric = evaluate.load('comet')\n",
    "    results_comet = comet_metric.compute(predictions=translation_infered, references=targets, sources=sources)\n",
    "\n",
    "    print(\"Computing METEOR...\")\n",
    "    meteor = evaluate.load('meteor')\n",
    "    results_meteor = meteor.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing Chrf++...\")\n",
    "    chrf = evaluate.load(\"chrf\")\n",
    "    results_chrf = chrf.compute(predictions=translation_infered, references=[[reference] for reference in targets])\n",
    "\n",
    "    print(\"Computing_bert_score...\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    results_bert = bertscore.compute(predictions=translation_infered, references=targets, lang=\"de\")\n",
    "    return {\n",
    "        \"bleu\": results_bleu,\n",
    "        \"rouge\": results_rouge,\n",
    "        \"bleurt\": results_bleurt,\n",
    "        \"sacrebleu\": results_sacrebleu,\n",
    "        \"comet\": results_comet,\n",
    "        \"meteor\": results_meteor,\n",
    "        \"chrf\": results_chrf,\n",
    "        \"bertscore\": results_bert,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & inference loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model.to(device)\n",
    "equivalence_language_to_FLORES = {\"en\": \"eng_Latn\", \"de\": \"deu_Latn\", \"ru\": \"rus_Cyrl\", \"is\": \"isl_Latn\", \"zh\": \"zho_Hans\", \"cz\": \"ces_Latn\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets_NLLB(dataset_wnt_format, source_lang, target_lang):\n",
    "    inputs = [example[source_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    return inputs, inputs, targets\n",
    "\n",
    "sources, inputs, targets = get_input_targets_NLLB(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jsem Američan.', 'Ahoj, jsem z Německa.']\n",
      "['Ég er bandarískur.', 'Ég kom frá Þýskalandi.']\n"
     ]
    }
   ],
   "source": [
    "# Translate text\n",
    "def translate_list_of_str_NLLB(list_str, tokenizer, model, to_laguage):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    equivalence_language_to_FLORES = {\"en\": \"eng_Latn\", \"de\": \"deu_Latn\", \"ru\": \"rus_Cyrl\", \"is\": \"isl_Latn\", \"zh\": \"zho_Hans\", \"cz\": \"ces_Latn\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        language_tgt_FLORES = equivalence_language_to_FLORES[to_laguage]\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(language_tgt_FLORES),\n",
    "                                    num_beams=5, max_length=512, early_stopping=True,\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_NLLB(inp, model, tokenizer, target_lang, batch_size):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=4\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inp)//batch_size+1)):\n",
    "        tslt = translate_list_of_str_NLLB(inp[i*batch_size : (i+1)*batch_size], tokenizer, model, target_lang)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "print(translate_list_of_str_NLLB([\"Hey! I am an american\", \"Hallo, Ich komme aus Deutschland\"], tokenizer, model, \"cz\"))\n",
    "print(translate_list_of_str_NLLB([\"Hey! I am an american\", \"Hallo, Ich komme aus Deutschland\"], tokenizer, model, \"is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [22:15<00:00,  9.54s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_trslt = translate_batched_NLLB(inputs, model, tokenizer, target_lang = \"de\", batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./translation_NLLB_en-de_save.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predicted_trslt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./translation_NLLB_en-de_save.pkl\", \"rb\") as f:\n",
    "    predicted_trslt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BLEU...\n",
      "Computing ROUGE...\n",
      "Computing BLEURT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SACREBLEU...\n",
      "Computing COMET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 18412.22it/s]\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "/home/dorian/miniconda3/envs/SNLP-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing METEOR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dorian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/dorian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/dorian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Chrf++...\n",
      "Computing_bert_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "evaluation = evaluate_translation(sources, targets, predicted_trslt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and LoRA weights\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cz\": \"Czech\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets_ALMA(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cz\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "sources, inputs, targets = get_input_targets_ALMA(ds, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate text\n",
    "def translate_list_of_str_ALMA(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=5, max_new_tokens=5, do_sample=True,\n",
    "                                    temperature=0.6, top_p=0.9\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_ALMA(inp, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=1\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inp)//batch_size+1)):\n",
    "        tslt = translate_list_of_str_ALMA(inp[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "print(translate_list_of_str_ALMA([\"Translate from English to Czech:\\nEnglish: Hey! I am an american \\nCzech:\",\n",
    "                                  \"Translate from German to Czech:\\nGerman: Hallo, Ich komme aus Deutschland \\nCzech:\"],\n",
    "                                  tokenizer, model, \"cz\"))\n",
    "print(translate_list_of_str_ALMA([\"Translate from English to Islandic:\\nEnglish: Hey! I am an american \\nIslandic:\",\n",
    "                                  \"Translate from German to Islandic:\\nGerman: Hallo, Ich komme aus Deutschland \\nIslandic:\"],\n",
    "                                  tokenizer, model, \"is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc61f75a60a43e09f297870203575bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Choose between 1B and 3B model\n",
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets_Llama3(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cz\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "sources, inputs, targets = get_input_targets_Llama3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_translation_Llama3(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|end_of_text|>\")[0]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Llama3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, return_tensors=\"pt\")\n",
    "        out_tokens = model.generate(**tokens.to(device),\n",
    "                                    num_beams=5, max_new_tokens=100, do_sample=True,\n",
    "                                    temperature=0.6, top_p=0.9)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Llama3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Llama3(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=20 with Llama3 1B, batch_size=4 with Llama3 3B\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size+1)):\n",
    "        tslt = translate_list_of_str_Llama3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Polizei verhaftet 15 nach gewaltsamen Protesten vor britischer Flüchtlingsunterkunft',\n",
       " 'Der Vorfall kommt nach einer Zunahme der Anzahl von Flüchtlingen und Asylsuchenden, die per Boot über den Kanal nach Großbritannien gelangen. Die Polizei hat 15 Personen festgenommen, nachdem eine Demonstration gegen Flüchtlinge vor einem Hotel, in dem Asylsuchende untergebracht werden, nahe der englischen Stadt Liverpool gewaltsam geworden ist. Die Polizeidienst',\n",
       " '<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|start_header_id|>assistant\\nDas Home Office nutzt das Hotel seit letzten Jahr als vorübergehende Unterkunft für Flüchtlinge, wie lokale Medien berichten. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend die Gemeinschaft nicht widerspiegelt. \"Die Bewohner von Knowsley sind keine Bigotten und begrüßen Menschen, die aus einigen der gefähr',\n",
       " '<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|start_header_id|>assistant\\nMehr als 45.000 Menschen erreichten das Vereinigte Königreich über diese Route im Jahr 2022, und die meisten beantragten Asyl. Das System zur Abwägung von Asylanträgen ist aufgrund politischer Unruhen und Verwaltungsverzögerungen zum Stillstand gekommen, sodass viele Flüchtlinge in Hotels oder anderen vorübergehenden Unterkünften stecken bleiben']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_list_of_str_Llama3(inputs[0:4], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General aggregation functions for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name == \"alma\":\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "        get_input_targets_fn = get_input_targets_ALMA\n",
    "        tslt_fn = translate_batched_ALMA\n",
    "    elif model_name == \"nllb\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        model.to(device)\n",
    "        get_input_targets_fn = get_input_targets_NLLB\n",
    "        tslt_fn = translate_batched_NLLB\n",
    "    elif model_name == \"llama3-1B\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        get_input_targets_fn = get_input_targets_Llama3\n",
    "        tslt_fn = translate_batched_Llama3\n",
    "    elif model_name == \"llama3-3B\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        get_input_targets_fn = get_input_targets_Llama3\n",
    "        tslt_fn = translate_batched_Llama3\n",
    "    return tokenizer, model, get_input_targets_fn, tslt_fn\n",
    "\n",
    "\n",
    "def eval_translation_different_directions(directions, dataset_name, model_name, batch_size):\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "\n",
    "    # Loading corresponding model\n",
    "    tokenizer, model, get_input_targets_fn, tslt_fn = load_model(model_name)\n",
    "\n",
    "    evaluation_all_direction = {}\n",
    "    for direction in directions:\n",
    "        input_language, target_language = direction[0:2], direction[3:5]\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "        # Extracting input & targets\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        translation_pred = tslt_fn(inputs, model, tokenizer, batch_size, target_language)\n",
    "        with open(f\"./generated_translations/evaluations/{dataset_name}_{model_name}_{direction}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(translation_pred, f, pickle.HIGHEST_PROTOCOL)\n",
    "        evaluation_all_direction[f\"{direction}\"] = evaluate_translation(sources, targets, translation_pred)\n",
    "    return evaluation_all_direction\n",
    "\n",
    "def eval_translation_general(directions, dataset_names, model_names, batch_size):\n",
    "    evaluation_all_datasets = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        evaluation_all_models = {}\n",
    "        for model_name in model_names:\n",
    "            evaluation_all_models[f\"{model_name}\"] = eval_translation_different_directions(directions, dataset_name, model_name, batch_size)\n",
    "        evaluation_all_datasets[f\"{dataset_name}\"] = evaluation_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
