{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 24 15:59:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A2                      Off |   00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   48C    P0             25W /   60W |       4MiB /  15356MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-24 15:59:57.919767: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 15:59:57.934378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740412797.952869   56824 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740412797.958301   56824 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 15:59:57.984633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import peft\n",
    "from datasets import Dataset\n",
    "import huggingface_hub\n",
    "from transformers import BitsAndBytesConfig\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 5\n",
    "max_new_tokens = 512\n",
    "top_p = 0.9\n",
    "temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tested = [\"en\", \"de\", \"cs\", \"is\", \"zh\", \"ru\"] # Only from or to english\n",
    "metrics_available = [\"bleu\", \"rouge\", \"bleurt\", \"sacrebleu\", \"comet\", \"meteor\", \"chrf\", \"bert_score\"]\n",
    "models_available = [\n",
    "    # NLLB\n",
    "    \"facebook/nllb-200-distilled-600M\",\n",
    "    # ALMA\n",
    "    \"haoranxu/ALMA-7B\",\n",
    "    # Llama 3 Instruct\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # Falcon 3 Mamba Instruct\n",
    "    \"tiiuae/Falcon3-Mamba-7B-Instruct\",\n",
    "    # Falcon 3 Instruct\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "    \"tiiuae/Falcon3-3B-Instruct\",\n",
    "    \"tiiuae/Falcon3-1B-Instruct\",\n",
    "    # Qwen 2.5 Mamba Instruct\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\",\n",
    "    # Mistral Instruct\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "] # TODO mistral 7B, Bloom 7B, OPT 7B, MPT 7B, Bayling (?)\n",
    "\n",
    "ds_available = [\"haoranxu/WMT23-Test\",\n",
    "                \"openlanguagedata/flores_plus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################   NLLB\n",
    "\n",
    "def get_input_targets_NLLB(dataset_wnt_format, source_lang, target_lang):\n",
    "    inputs = [example[source_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    return inputs, inputs, targets\n",
    "\n",
    "def translate_list_of_str_NLLB(list_str, tokenizer, model, to_laguage):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    equivalence_language_to_FLORES = {\"en\": \"eng_Latn\", \"de\": \"deu_Latn\", \"ru\": \"rus_Cyrl\", \"is\": \"isl_Latn\", \"zh\": \"zho_Hans\", \"cs\": \"ces_Latn\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        language_tgt_FLORES = equivalence_language_to_FLORES[to_laguage]\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(language_tgt_FLORES),\n",
    "                                    num_beams=5, max_length=512, early_stopping=True,\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_NLLB(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size = 4\n",
    "    For 16GB VRAM, use batch_size = 8 (better working with unbatch version to avoid pad noise).\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_NLLB(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   ALMA\n",
    "\n",
    "def get_input_targets_ALMA(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_ALMA(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=5, max_new_tokens=512, do_sample=True,\n",
    "                                    temperature=0.6, top_p=0.9\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_ALMA(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=1\n",
    "    For 16GB VRAM, use batch_size=3\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_ALMA(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Llama 3\n",
    "\n",
    "def get_input_targets_Llama3(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "        \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Llama3(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|end_of_text|>\")[0]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>assistant\\n\")[-1]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>\")[-1]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Llama3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\")\n",
    "        out_tokens = model.generate(**tokens.to(device),\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Llama3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Llama3(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM use\n",
    "        batch_size=20 with Llama3 1B,\n",
    "        batch_size=4 with Llama3 3B\n",
    "    For 16 GB VRAM use \n",
    "        batch_size=40 with Llama3 1B,\n",
    "        batch_size=10 with Llama3 3B,\n",
    "        batch_size=5 with Llama3 8B,\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Llama3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language=None)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Falcon 3 (Normal + Mamba)\n",
    "\n",
    "def get_input_targets_Falcon3(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Falcon3Mamba(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|im_end|>\\n<|im_start|>assistant\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|im_end|>\")[0]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Falcon3Mamba(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=5, do_sample=True,\n",
    "                                    temperature=0.6, top_p=0.9, max_new_tokens=300)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3Mamba(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3Mamba(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=4 with Falcon Mamba 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon Mamba 7B (4 bits quantization),\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3Mamba(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "def extract_translation_Falcon3(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|assistant|>\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|pad|>\")[-1]\n",
    "    translation_only = answer.split(\"<|endoftext|>\")[0]\n",
    "    translation_only = re.sub(r\"^[^a-zA-Z0-9]*\", \"\", translation_only)\n",
    "    return translation_only.replace(\"assistant|>\\n\", \"\")\n",
    "\n",
    "def translate_list_of_str_Falcon3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=5, do_sample=True,\n",
    "                                    temperature=0.6, top_p=0.9, max_new_tokens=500)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=8 with Falcon 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon 3B,\n",
    "        batch_size=12 with Falcon 1B\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Qwen 2.5\n",
    "\n",
    "def get_input_targets_Qwen2_5(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Qwen2_5(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|im_start|>assistant\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|im_end|>\")[0]\n",
    "    translation_only = answer.replace(\"<|endoftext|>\", \"\")\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Qwen2_5(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Qwen2_5(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Qwen2_5(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=100 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Qwen2_5(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Mistral\n",
    "\n",
    "\n",
    "def get_input_targets_Mistral(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Mistral(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"[/INST] \")[-1]\n",
    "    answer = answerpadded.split(\"</s>\")[0]\n",
    "    return answer\n",
    "\n",
    "def translate_list_of_str_Mistral(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Mistral(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Mistral(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=2 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Mistral(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling\n",
    "We use WNT23 from the authors preprocessed split and the FLORES+ dataset, format in the same way that the WNT23 is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNT23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557 {'en-de': [{'en': 'Police arrest 15 after violent protest outside UK refugee hotel', 'de': 'Polizei verhaftet 15 Menschen nach gewalttätigen Protesten vor einer Flüchtlingsunterkunft in Großbritannien'}, {'en': 'The incident comes after increase in numbers of refugees and asylum seekers crossing the Channel to the UK in boats. Police have arrested 15 people after an anti-refugee demonstration outside a hotel used to house asylum seekers turned violent near the English city of Liverpool. The Merseyside Police department said a police officer and two civilians sustained minor injuries during the disturbance on Friday night in Knowsley. The police force said some protesters threw objects and set a police van on fire. The people arrested, who ranged in age from 13 to 54, were detained \"following violent disorder.\" Merseyside police commissioner Emily Spurrell told Radio City, \"It was incredibly dangerous and there were a couple of injuries amongst the police officers.\"', 'de': 'Der Vorfall ereignet sich, nachdem sich die Zahl der Flüchtlinge und Asylbewerber, die den Ärmelkanal nach Großbritannien in Booten überqueren, ansteigt. Die Polizei hat 15 Menschen verhaftet, nachdem Anti-Flüchtlingsdemonstrationen vor einem Hotel in der Nähe der englischen Stadt Liverpool, das verwendet wird, um Asylbewerber unterzubringen, gewalttätig wurden. Laut dem Polizeirevier von Merseyside erlitten ein Polizeibeamter und zwei Zivilisten während des Tumults am Freitagabend in Knowsley leichte Verletzungen. Die Polizei gab bekannt, dass einige Demonstranten Gegenstände warfen und einen Polizeiwagen anzündeten. Die verhafteten Menschen, die zwischen 13 und 54 Jahren alt waren, wurden „aufgrund von gewalttätigen Ausschreitungen“ inhaftiert. Die Polizeikommissarin von Merseyside Emily Spurrell äußerte gegenüber Radio City, „Es war unglaublich gefährlich und es gab einige Verletzte unter den Polizeibeamten.“'}, {'en': 'The Home Office has been using the hotel to temporarily house asylum seekers since last year, according to local media. George Howarth, who represents Knowsley in the UK Parliament, said the violence on Friday night did not reflect the community. \"The people of Knowsley are not bigots and are welcoming to people escaping from some of the most dangerous places in the world in search of a place of safety,\" he said. \"Those demonstrating against refugees at this protest tonight do not represent this community.\" The protest took place amid heightened tensions as growing numbers of refugees and migrants cross the Channel in small boats.', 'de': 'Das Innenministerium hat das Hotel seit letztem Jahr verwendet, um vorübergehend Asylbewerber unterzubringen, so die lokalen Medien. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend nicht die Gemeinde widerspiegelt. „Die Einwohner von Knowsley sind keine Rassisten und sind gastfreundlich gegenüber Menschen, die von einigen der gefährlichsten Orten der Welt auf der Suche nach einem sicheren Ort fliehen“, sagte er. „Diejenigen, die bei heute Abend bei diesem Protest demonstrieren, spiegeln diese Gemeinde nicht wider.“ Der Protest fand unter erhöhten Spannungen statt, während wachsende Zahlen von Flüchtlingen und Migranten den Ärmelkanal in kleinen Booten überqueren.'}, {'en': 'More than 45,000 people reached the UK by that route in 2022, and most applied for asylum. The system for considering asylum applications has slowed to a crawl because of political turmoil and bureaucratic delays, leaving many asylum seekers stuck in hotels or other temporary accommodations. The Channel crossings have become a political issue, with the Conservative government promising to \"stop the boats\" and pursuing a plan to send such asylum seekers to Rwanda. Opponents have accused the government of demonising desperate people fleeing war and poverty.', 'de': 'Mehr als 45.000 Menschen haben 2022 Großbritannien auf diesem Weg erreicht und die meisten haben einen Asylantrag gestellt. Das System, das Asylanträge bearbeitet, hat sich aufgrund politischer Unruhen und bürokratischer Verzögerungen stark verlangsamt, was dazu führte, dass viele Asylbewerber in Hotels und vorübergehenden Unterkünften feststecken. Die Überquerungen des Ärmelkanals sind zu einem politischen Thema geworden, die konservative Regierungen verspricht hierbei „die Boote zu stoppen“ und verfolgt einen Plan, jene Asylbewerber nach Ruanda zu schicken. Gegner haben der Regierung vorgeworfen, verzweifelte Menschen, die vor Krieg und Armut fliehen, zu dämonisieren.'}]}\n"
     ]
    }
   ],
   "source": [
    "ds_wnt = load_dataset(\"haoranxu/WMT23-Test\", \"en-de\")[\"test\"]\n",
    "print(len(ds_wnt), ds_wnt[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bedb4a6e38340b99c3c2bec9059af7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a04860df9a54201be2c0827705e8757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6dcd023a0f4705a1598b52b48f25c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0752df7916d54b5cbc79f7fc93f9531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_WNT_style(ds_flores, lang, lang_start=\"en\"):\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"zho\", \"ru\": \"rus\"}\n",
    "    list_sentence_lang, list_sentence_lang_start = [], []\n",
    "    for elem in ds_flores:\n",
    "        if elem['iso_639_3'] == language_to_iso[lang]:\n",
    "            list_sentence_lang.append(elem[\"text\"])\n",
    "        elif elem['iso_639_3'] == language_to_iso[lang_start]:\n",
    "            list_sentence_lang_start.append(elem[\"text\"])\n",
    "    assert len(list_sentence_lang) == len(list_sentence_lang_start)\n",
    "    print(f\"Number of samples: {len(list_sentence_lang)}\")\n",
    "    final_text_list = []\n",
    "    for i in range(len(list_sentence_lang)):\n",
    "        final_text_list.append({f\"{lang_start}\": list_sentence_lang_start[i],\n",
    "                                f\"{lang}\": list_sentence_lang[i],})\n",
    "    return Dataset.from_dict({f\"{lang_start}-{lang}\": final_text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_to_WNT_style' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_flores_wnt_style \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_to_WNT_style\u001b[49m(ds_flores, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ds_flores_wnt_style[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform_to_WNT_style' is not defined"
     ]
    }
   ],
   "source": [
    "ds_flores_wnt_style = transform_to_WNT_style(ds_flores, lang=\"de\", lang_start=\"en\")\n",
    "print(ds_flores_wnt_style[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from predictions: evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(sources, targets, translation_infered):\n",
    "    print(\"Computing BLEU...\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results_bleu = bleu.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing ROUGE...\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results_rouge = rouge.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing BLEURT...\")\n",
    "    bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "    results_bleurt = bleurt.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing SACREBLEU...\")\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "    results_sacrebleu = sacrebleu.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing COMET...\")\n",
    "    comet_metric = evaluate.load('comet')\n",
    "    results_comet = comet_metric.compute(predictions=translation_infered, references=targets, sources=sources)\n",
    "\n",
    "    print(\"Computing METEOR...\")\n",
    "    meteor = evaluate.load('meteor')\n",
    "    results_meteor = meteor.compute(predictions=translation_infered, references=targets)\n",
    "\n",
    "    print(\"Computing Chrf++...\")\n",
    "    chrf = evaluate.load(\"chrf\")\n",
    "    results_chrf = chrf.compute(predictions=translation_infered, references=[[reference] for reference in targets])\n",
    "\n",
    "    print(\"Computing_bert_score...\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    results_bert = bertscore.compute(predictions=translation_infered, references=targets, lang=\"de\")\n",
    "    return {\n",
    "        \"bleu\": results_bleu,\n",
    "        \"rouge\": results_rouge,\n",
    "        \"bleurt\": results_bleurt,\n",
    "        \"sacrebleu\": results_sacrebleu,\n",
    "        \"comet\": results_comet,\n",
    "        \"meteor\": results_meteor,\n",
    "        \"chrf\": results_chrf,\n",
    "        \"bertscore\": results_bert,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & inference loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_NLLB(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polizei verhaftet 15 Personen nach gewalttätigen Protesten vor einem Flüchtlingshotel in Großbritannien']\n",
      "['Die Polizei hat 15 Personen verhaftet, nachdem eine Anti-Flüchtlingsdemonstration vor einem Hotel, in dem Asylsuchende untergebracht wurden, in der Nähe der englischen Stadt Liverpool gewalttätig wurde. Die Polizeibehörde von Merseyside sagte, dass ein Polizist und zwei Zivilisten während der Unruhe am Freitagabend in Knowsley leicht verletzt wurden. Die Polizei sagte, dass einige Demonstranten Gegenstände geworfen und einen Polizeiwagen in Brand gesetzt hatten. Die festgenommenen Personen, die im Alter von 13 bis 54 Jahren waren, wurden \"nach gewalttätiger Unordnung\" festgenommen. \"Es war unglaublich gefährlich und es gab ein paar Verletzungen unter den Polizisten\", sagte die Polizeibeauftragte von Merseyside Emily Spurrell gegenüber Radio City.']\n",
      "['Das Innenministerium nutzt das Hotel seit dem vergangenen Jahr, um Asylsuchende vorübergehend zu beherbergen, berichten lokale Medien. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend nicht die Gemeinschaft widerspiegelt. \"Die Menschen in Knowsley sind nicht bigotisch und begrüßen Menschen, die aus einigen der gefährlichsten Orte der Welt in der Suche nach einem sicheren Ort fliehen\", sagte er. \"Diejenigen, die gegen Flüchtlinge auf diesem Protest heute Abend demonstrieren, repräsentieren diese Gemeinschaft nicht\". Der Protest fand inmitten der erhöhten Spannungen statt, da zunehmend Flüchtlinge und Migranten in kleinen Booten den Kanal überqueren.']\n",
      "['Mehr als 45.000 Menschen erreichten das Vereinigte Königreich auf dieser Strecke im Jahr 2022, und die meisten beantragten Asyl. Das Asylantragssystem hat sich aufgrund politischer Unruhen und bürokratischer Verzögerungen zu einer Verzögerung verlangsamt, so dass viele Asylsuchende in Hotels oder anderen temporären Unterkünften stecken bleiben. Die Kanalüberfahrten sind zu einem politischen Thema geworden, wobei die konservative Regierung versprach, \"die Boote zu stoppen\" und einen Plan zu verfolgen, um solche Asylsuchenden nach Ruanda zu schicken.']\n",
      "['Präsidentencup: Candystripes besiegt Rovers in der Saisonöffnung in Brandywell']\n",
      "['Derry Mittelfeldspieler Adam Reilly schließt Lee Grace im Brandywell ab']\n",
      "[\"Derry City gewann den President's Cup, als sie den Shamrock Rovers mit 2-0 besiegten. Die FAI Cup-Halter besiegten die Liga-Gewinner der letzten Saison im Brandywell dank der ersten Hälfte von Will Patching und Michael Duffy. Die Hoops drängten in der zweiten Hälfte auf reduzierte Bedingungen, aber die Anklagen von Ruaidhri Higgins standen fest, um den Vorhang zu gewinnen. Derry reist zum St. Patrick's Athletic für den Liga-Offener am nächsten Freitag.\"]\n",
      "['Die Hoops gewannen die Premier Division mit 13 Punkten, aber Duffy brachte City bald mit einer langfristigen Bemühung weiter voran, die es geschafft hat, sich unter Keeper Leon Pohls und in das Netz zu verkrampfen. Die zweitrangigen Teilnehmer der letzten Saison waren zwei Tore, um in der Pause gut zu sein. Graham Burke kam am nächsten, um die Rückstände in der zweiten Periode zu reduzieren, aber die Candystripes waren eine Woche vor ihrem Streben nach nationaler Ruhm komfortabeler Gewinner. City-Chef Ruaidhri Higgins lobte sein Team nach dem, was er sagte, war die \"schwierigste Woche meines Lebens\" nach dem Tod seines Bruders Kevin. \"Es ist ein Kick in die Zähne und es gibt schwere Wochen, aber wir werden mit ihm in seinem Gedächtnis weitermachen\", sagte Higgins. \"Shamrock Rovers zu schlagen und verdient so mit einer guten Leistung zu schlagen ist wirklich angenehmlich\".']\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(translate_list_of_str_NLLB(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_trslt = translate_batched_NLLB(inputs[0:2], model, tokenizer, target_lang = \"de\", batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./translation_NLLB_en-de_save.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predicted_trslt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./translation_NLLB_en-de_save.pkl\", \"rb\") as f:\n",
    "    predicted_trslt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BLEU...\n",
      "Computing ROUGE...\n",
      "Computing BLEURT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/onyxia/.cache/huggingface/metrics/bleurt/default/downloads/extracted/ec99b4b83def7843831e88a47af3e7c90f3315b30f2a6acb70a5587bac0264e0/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/onyxia/.cache/huggingface/metrics/bleurt/default/downloads/extracted/ec99b4b83def7843831e88a47af3e7c90f3315b30f2a6acb70a5587bac0264e0/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SACREBLEU...\n",
      "Computing COMET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 31207.62it/s]\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "/opt/conda/envs/SNLP-gpu/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing METEOR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Chrf++...\n",
      "Computing_bert_score...\n"
     ]
    }
   ],
   "source": [
    "evaluation = evaluate_translation(sources[0:2], targets[0:2], predicted_trslt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model and LoRA weights\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_ALMA(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/SNLP-gpu/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polizei nimmt 15 Personen nach gewalttätigen Protesten außerhalb eines ukrainischen Flüchtlingshotels fest']\n",
      "['Der Vorfall ereignete sich nach einem Anstieg der Zahl von Flüchtlingen und Asylbewerbern, die mit Booten über den Kanal nach Großbritannien übersetzten. Die Polizei hat 15 Personen nach einer anti-Flüchtlings-Demonstration außerhalb eines Hotels festgenommen, in dem Asylbewerber untergebracht waren. Das Polizeipräsidium Merseyside teilte mit, dass ein Polizeibeamter und zwei Zivilisten leichte Verletzungen erlitten hatten während der Auseinandersetzung am Freitagabend in Knowsley. Die Polizeibehörde teilte mit, einige Demonstranten hätten Gegenstände geworfen und einen Polizeiwagen in Brand gesteckt. Die festgenommenen Personen, die zwischen 13 und 54 Jahre alt waren, wurden \"nach gewalttätigen Ausschreitungen\" verhaftet. Die Polizeipräsidentin von Merseyside, Emily Spurrell, sagte im Radio City: \"Es war unglaublich gefährlich und es gab einige Verletzungen unter den Polizeibeamten.\"']\n",
      "['Demnach nutzt das Innenministerium das Hotel seit letztem Jahr vorübergehend als Unterkunft für Asylbewerber, wie die örtlichen Medien berichten. Der Abgeordnete George Howarth, der Knowsley im britischen Unterhaus repräsentiert, sagte, dass die Gewalt am Freitagabend nicht die Gemeinschaft widerspiegelt. \"Die Menschen in Knowsley sind keine Rassisten und sind willkommen gegenüber Menschen, die aus den gefährlichsten Orten der Welt fliehen und einen sicheren Ort suchen\", sagte er. \"Die Demonstranten, die heute Abend gegen Flüchtlinge demonstrieren, repräsentieren diese Gemeinschaft nicht.\" Der Protest fand unter erhöhten Spannungen statt aufgrund der wachsenden Anzahl von Flüchtlingen und Migranten, die mit kleinen Booten über den Kanal kommen.']\n",
      "['Im Jahr 2022 erreichten mehr als 45 000 Menschen das Vereinigte Königreich über diesen Weg und die meisten von ihnen beantragten Asyl. Das System für die Prüfung von Asylanträgen ist aufgrund politischer Turbulenzen und bürokratischer Verzögerungen zum Erliegen gekommen, wodurch viele Asylbewerber in Hotels oder anderen vorläufigen Unterkünften festhängen. Die Überfahrten über den Kanal haben sich zu einem politischen Thema entwickelt und die konservative Regierung verspricht, \"die Boote zu stoppen\" und verfolgt einen Plan, Asylbewerber nach Ruanda zu senden. Gegner haben die Regierung beschuldigt, Flüchtlinge, die vor Krieg und Armut fliehen, zu verunglimpfen.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_ALMA(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/4 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=\"auto\", device_map=device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m Q_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/transformers/modeling_utils.py:4022\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   4020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   4021\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 4022\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4035\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4038\u001b[0m     is_safetensors_available()\n\u001b[1;32m   4039\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   4040\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4041\u001b[0m ):\n\u001b[1;32m   4042\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/transformers/utils/hub.py:1037\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/huggingface_hub/file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/huggingface_hub/file_download.py:1547\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1545\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1547\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1557\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/huggingface_hub/file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    456\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/http/client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/envs/SNLP-project/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Llama3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/opt/conda/envs/SNLP-gpu/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Polizei verhaftet 15 nach gewaltsamen Protesten vor einem Flüchtlingshotel in Großbritannien',\n",
       " 'Das Ereignis kommt nach einem Anstieg der Zahlen von Flüchtlingen und Asylsuchenden, die in Booten über den Kanal in das Vereinigte Königreich einreisen. Die Polizei hat 15 Menschen festgenommen, nachdem eine Anti-Flüchtlingsdemonstration vor einem Hotel, das als Unterkunft für Asylsuchende diente, in der Nähe der englischen Stadt Liverpool gewalttätig wurde. Die Polizeibehörde von Merseyside sagte, ein Polizeibeamter und zwei Zivilisten seien bei dem Aufruhr am Freitagabend in Knowsley leicht verletzt worden. Die Polizei sagte, einige Demonstranten hätten Gegenstände geworfen und ein Polizeifahrzeug in Brand gesteckt. Die festgenommenen Personen, die im Alter von 13 bis 54 Jahre alt waren, wurden \"nach gewalttätigem Verhalten\" festgenommen. Die Polizeikommissarin von Merseyside, Emily Spurrell, sagte bei Radio City, \"Es war äußerst gefährlich und es gab einige Verletzungen unter den Polizeibeamten.\"',\n",
       " 'Das Home Office nutzt das Hotel seit letztem Jahr, um Asylsuchende vorübergehend unterzubringen, wie lokale Medien berichten. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend die Gemeinschaft nicht widerspiegelt. \"Die Menschen von Knowsley sind keine Bigotten und sind willkommen für Menschen, die aus einigen der gefährlichsten Orten der Welt fliehen, in der Suche nach einem sicheren Ort\", sagte er. \"Diejenigen, die gegen Flüchtlinge demonstrieren, stellen diese Gemeinschaft nicht dar.\" Die Demonstration fand unter erhöhten Spannungen statt, als wachsende Zahlen von Flüchtlingen und Migranten den Kanal in kleinen Booten überqueren.',\n",
       " 'Mehr als 45.000 Menschen kamen 2022 über diese Route in das Vereinigte Königreich an und die meisten stellten einen Asylantrag. Das System zur Bearbeitung von Asylanträgen ist wegen politischer Turbulenzen und bürokratischer Verzögerungen zum Erliegen gekommen, sodass viele Asylsuchende in Hotels oder anderen vorübergehenden Unterkünften stecken bleiben. Die Überfahrten über den Kanal sind zu einem politischen Thema geworden, mit der konservativen Regierung, die verspricht, \"die Boote aufzuhalten\" und einen Plan verfolgt, Asylsuchende nach Ruanda zu schicken. Kritiker werfen der Regierung vor, Menschen, die vor Krieg und Armut fliehen, zu demonisieren.',\n",
       " \"President's Cup: Candystripes besiegen Rovers in Saisonauftakt im Brandywell\",\n",
       " 'Derry-Mittelfeldspieler Adam Reilly deckt Lee Grace im Brandywell ab.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_list_of_str_Llama3(inputs[0:5], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon 3 Instruct (mamba and transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Falcon3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Falcon3Mamba(inputs[0:4], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/opt/conda/envs/SNLP-gpu/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "translate_list_of_str_Falcon3(inputs[0:8], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Qwen2_5(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polizei arrebten 15 nach einer heftigen Protestzeiten in einem UK-reflektierenden Hotel']\n",
      "['Das Ereignis folgt einer steigenden Anzahl von Flüchtlingen und Asylbewohnern, die den Atlantik über den England nach Großbritannien fliegen. Die Polizei hat 15 Menschen verhaftet, nachdem ein Anti-Refugee-Demonstration im Hotel, wo Asylbewohner wohnen, in der Nähe der Stadt Liverpool ungemacht wurde. Der Merseyside Polizeidirektor Emily Spurrell sagte am Radio City: \"Es war sehr gefährlich und es gab ein paar Verletzungen bei den Polizisten.\"']\n",
      "['Der Staatsanwalt hat seit dem letzten Jahr das Hotel für Asylbewerber benutzt, um sie zu暂时erhalten. George Howarth, der im UK-Parlament innehat, sagte, dass die Begegnung am Freitag morgens nicht die Gemeinschaft repräsentiert. \"Die Menschen von Knowsley sind nicht bigots und sind freundlich zu Menschen, die aus vielen der gefährlichsten Städten in der Welt nach einem Ort der Sicherheit suchen,\" sagte er. \"Diese Menschen, die gegen Asylbewerber protestieren, sind nicht die Gemeinschaft.\" Die Proteste, die am Morgen stattgefunden haben, während die Anzahl von Asylbewerbern und Migranten, die in kleinen Booten über den Atlantik fliegen, zunehmend gestiegen sind.']\n",
      "['Im Jahr 2022 wurden über 45. 000 Menschen nach Großbritannien gekommen, und mehr als ein Dutzend applaudigten für Asyl. Die Systeme für Asylanzeugnisse sind zu schwierig geworden, da der politische Chaos und bessere Verwaltungskräfte dazu geführt haben, die Asylanzeugnisse zu beenden. Der Kanalcross ist eine politische Sache, bei der die Conservative-Gemeinschaft versprochen hat, \"das Boot zu stoppen\" und einen Plan zu entwickeln, Asylanzeugnisse in Rwanda zu senden. Einige Gegner der Regierung haben sich gegen die Regierung beschuldigt, Asylanzeugnisse zu demonisieren, die durch Krieg und Schäden gefördert wurden.']\n",
      "[\"President's Cup: Candystripes verlieren Rovers im Spielbeginn bei Brandywell\"]\n",
      "['Derry-Midfielder Adam Reilly vermeidet Lee Grace bei der Brandywell.']\n",
      "[\"Derry City gewann im Presidenten-Kup als Sieger 2-0 über Shamrock Rovers. Die FAI-Cup-Verdienstschlüssel gewannen die Brandywell mit den ersten Halbfohlen von Will Patching und Michael Duffy. Die Hoops versuchten zu reduzieren, aber Ruaidhri Higgins' Vorschläge standen fest, um die Kürze zu gewinnen. Derry zieht sich nach St. Patrick's Athletic für den Start der Liga. Der Foylesider spuckte einen Schlag gegen Derry vor 23 Minuten - der Mittelfelder öffnete das Spiel als die Foylesider den 2-1 Winners über Shamrock Rovers ausgetragen haben.\"]\n",
      "['Die Hoops gewann die Premier Division durch 13 Punkte, aber Duffy brachte City näher vor, indem er einen langfristigen Angriff, den Leon Pohls in die Eingangstür gebracht hatte, in den Netton. In der letzten Saison waren die Runner-ups zwei Punkte zu gut. Graham Burke war näher gekommen, um die Arretsverluste in der zweiten Phase zu reduzieren, aber die Candystripes waren glücklich gewesen vor einem Monat vor ihrem Aufstieg in die Nationalmannschaft. Der Boss Ruaidhri Higgins berichtete über seine Team nach, was er als \"der hardesten Monat seines Lebens\" empfand, nachdem sein Bruder Kevin gestorben war. \"Es ist ein Schlag in die Kehle und es gibt Tough Wege vor uns, aber wir werden weitergehen in seinem Geist,\" sagte Higgins. \"Um Shamrock Rovers zu gewinnen und rechtzeitig zu gewinnen, war ein guter Angriff.\"']\n",
      "['DHS empfängt einen Lawfirma, um mögliche Alejandro Mayorkas-Impeachment-Prozesse zu handlengen.']\n",
      "['Die Sicherheitsdienststelle hat einen Auswanderungsprozessberater hinzugefügt - DeBevoise & Plimpton - um mögliche Impeachmentprozesse für den Sekretär Alejandro Mayorkas zu assistieren, der möglicherweise im Hinblick auf seine Behandlung des südamerikanischen Grenzüberschritts angesprochen wird. \"Die Sicherheitsdienststelle hat einen Auswanderungsprozessberater hinzugefügt - DeBevoise & Plimpton - um mögliche Impeachmentprozesse für den Sekretär Alejandro Mayorkas zu assistieren, der möglicherweise im Hinblick auf seine Behandlung des südamerikanischen Grenzüberschritts angesprochen wird. \"DeBevoise & Plimpton wird die Sicherheitsdienststelle weiterhin prioritär prioritieren, um die Sicherheit unseres Landes vor Terrorismus zu schützen, die Naturdisasters zu begegnen und unsere Grenzüberschrittsanlagen zu schützen, während wir rechtmäßige Aufgaben für die über 70 Komitee und Subkomitee der Sicherheitsdienststelle erfüllen.\" Eine Sicherheitsdienststelle sagt CBS News: \"Die Sicherheitsdienststelle hat einen Auswanderungsprozessberater hinzugefügt - DeBevoise & Plimpton - um mögliche Impeachmentprozesse für den Sekretär Alejandro Mayorkas zu assistieren, der möglicherweise im Hinblick auf seine Behandlung des südamerikanischen Grenzüberschritts angesprochen wird. \"Die Sicherheitsdienststelle hat einen Auswanderungsprozessberater hinzugefügt - DeBevoise & Plimpton - um mögliche Impeachmentprozesse für den Sekretär Alejandro Mayorkas zu assistieren, der möglicherweise im Hinblick auf seine Behandlung des südamerikanischen Grenzüberschritts angesprochen wird. \"DeBevoise & Plimpton wird die Sicherheitsdienststelle weiterhin prioritär prioritieren, um die Sicherheit unseres Landes vor Terrorismus zu schützen']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(translate_list_of_str_Qwen2_5(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [05:45<00:00, 115.15s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Mistral(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Polizei verhaftet 15 nach gewalttätigen Protesten vor Flüchtlingshotel im Vereinigten Königreich.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das Ereignis ereignete sich nach dem Anstieg der Zahl von Flüchtlingen und Asylsuchenden, die über den Kanal nach Großbritannien in Booten gelangten. Die Polizei hat 15 Personen festgenommen, nachdem eine antiflüchtlingsdemonstration vor einem Hotel, das Asylsuchende unterkunftlich macht, gewalttätig geworden ist, in der Nähe der englischen Stadt Liverpool. Das Polizeidepartement Merseyside erklärte, dass ein Polizeibeamter und zwei Zivilisten leichte Verletzungen erlitten haben, während die Störung am Freitagabend in Knowsley stattfand. Die Polizeibehörde erklärte, dass einige Demonstranten Gegenstände warfen und ein Polizeifahrzeug anzündeten. Die festgenommenen Personen, deren Alter sich zwischen 13 und 54 Jahren befand, wurden \"nach gewalttätigem Verhalten\" festgehalten. Die Polizeichefin von Merseyside, Emily Spurrell, erzählte Radio City, \"Es war sehr gefährlich und es gab einige Verletzungen unter den Polizeibeamten.\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das Home Office nutzt seit letztem Jahr ein Hotel, um Asylsuchende vorübergehend unterzubringen, laut lokalen Medien. George Howarth, der Knowsley im britischen Parlament vertritt, sagte, dass die Gewalt am Freitagabend nicht den Charakter der Gemeinde widerspiegelte. \"Die Leute von Knowsley sind nicht Rassisten und freundlich gegenüber Menschen, die aus einigen der gefährlichsten Orte der Welt fliehen, um sich einen sicheren Ort zu suchen.\" Diejenigen, die gegen Flüchtlinge bei dieser Demonstration demonstrierten, seien nicht die Vertreter dieser Gemeinde. Die Demonstration fand unter erhöhten Spannungen statt, als sich zunehmende Zahlen von Flüchtlingen und Migranten über den Kanal in kleinen Booten begeben.']\n",
      "['Mehr als 45.000 Menschen erreichten das Vereinigte Königreich über diesen Weg im Jahr 2022, und die meisten meldeten sich als Flüchtlinge an. Das System zur Prüfung von Flüchtlingsanträgen ist aufgrund politischer Unruhen und administrativer Verspätungen fast zum Stillstand gekommen, was viele Flüchtlinge in Hotels oder anderen vorübergehenden Unterkünften stecken lässt. Die Kanalüberquerungen haben sich zu einer politischen Frage gewandelt, mit der konservativen Regierung verspricht, die Boote zu stoppen, und verfolgt einen Plan, solche Flüchtlinge nach Rwanda zu schicken. Gegner haben die Regierung beschuldigt, Flüchtlinge, die aus Kriegen und Armut fliehen, zu verunglimpfen.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_Mistral(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\"role\": \"system\", \"content\": \"Translate from English to Czech:\\nEnglish: Hey! I am an american \\nCzech:\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"Translate from German to Czech:\\nGerman: Hallo, Ich komme aus Deutschland \\nCzech:\"}]\n",
    "]\n",
    "instruct_messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "tokens = tokenizer(instruct_messages, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "print(tokens)\n",
    "print(instruct_messages)\n",
    "with torch.no_grad():    \n",
    "    out_tokens = model.generate(**tokens,\n",
    "                                num_beams=5, max_new_tokens=100, do_sample=True,\n",
    "                                temperature=0.6, top_p=0.9)\n",
    "translations = tokenizer.batch_decode(out_tokens)\n",
    "translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General aggregation functions for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_size=None):\n",
    "    if model_name == \"alma\":\n",
    "        tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        get_input_targets_fn = get_input_targets_ALMA\n",
    "        tslt_fn = translate_batched_ALMA\n",
    "        \n",
    "    elif model_name == \"nllb\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)\n",
    "        get_input_targets_fn = get_input_targets_NLLB\n",
    "        tslt_fn = translate_batched_NLLB\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        get_input_targets_fn = get_input_targets_Llama3\n",
    "        tslt_fn = translate_batched_Llama3\n",
    "    \n",
    "    elif model_name == \"falcon3-mamba\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3Mamba\n",
    "    \n",
    "    elif model_name == \"falcon3\":\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)  \n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3\n",
    "    \n",
    "    elif model_name == \"qwen2.5\":\n",
    "        if model_size==\"0.5B\" or model_size==\"1.5B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\", torch_dtype=\"auto\", device_map=device)\n",
    "        get_input_targets_fn = get_input_targets_Qwen2_5\n",
    "        tslt_fn = translate_batched_Qwen2_5\n",
    "    \n",
    "    elif model_name == \"mistral\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        get_input_targets_fn = get_input_targets_Mistral\n",
    "        tslt_fn = translate_batched_Mistral\n",
    "        \n",
    "    return tokenizer, model, get_input_targets_fn, tslt_fn\n",
    "\n",
    "\n",
    "def reduce_dataset(inputs, sources, targets, final_nb):\n",
    "    idx = np.arange(len(inputs))\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(idx, final_nb)\n",
    "    return [inputs[i] for i in idx], [sources[i] for i in idx], [targets[i] for i in idx]\n",
    "\n",
    "\n",
    "def generate_translation_different_directions(directions, dataset_name, model_name, batch_size, reduce_size = None, model_size = None):\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "\n",
    "    # Loading corresponding model\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer, model, get_input_targets_fn, tslt_fn = load_model(model_name, model_size)\n",
    "\n",
    "    for direction in directions:\n",
    "        print(f\"Translating {direction} with model {model_name} for dataset {dataset_name}...\")\n",
    "        input_language, target_language = direction[0:2], direction[3:5]\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            if direction != \"cs-en\":\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "            else:\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "                ds = Dataset.from_dict({f\"cs-en\": ds[\"en-cs\"][::-1]}) # Reverse list to avoid having same sentences (if reduce_size not None)\n",
    "        # Extracting input & targets\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        print(f\"Total number of samples: {len(sources)}\" + (\"\" if reduce_size is None else f\"; reduced to {reduce_size} (numpy seed = 42)\"))\n",
    "        if reduce_size is not None:\n",
    "            sources, inputs, targets = reduce_dataset(sources, inputs, targets, reduce_size)\n",
    "        translation_pred = tslt_fn(inputs, model, tokenizer, batch_size, target_language)\n",
    "        if not os.path.exists(f\"./generated_translations/evaluations\"):\n",
    "            os.makedirs(f\"./generated_translations/evaluations\")\n",
    "        with open(f\"./generated_translations/evaluations/{dataset_name}_{model_name}_{direction}_red-{reduce_size}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(translation_pred, f, pickle.HIGHEST_PROTOCOL)\n",
    "    model.cpu()\n",
    "    del model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model alma for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model alma for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model alma for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model alma for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model alma for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model alma for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model alma for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model alma for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model alma for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model alma for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"alma\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Translating en-de with model llama3 for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model llama3 for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model llama3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model llama3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model llama3 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model llama3 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model llama3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model llama3 for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model llama3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model llama3 for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"llama3\",\n",
    "                                          model_size=\"1B\",\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [07:01<00:00, 140.48s/it]\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]The 'batch_size' argument of MambaCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:40<00:00, 20.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:18<00:00, 39.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:16<00:00, 38.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:42<00:00, 21.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model falcon3-mamba for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.37s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"falcon3-mamba\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Translating en-de with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model falcon3 for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"falcon3\",\n",
    "                                          model_size=\"1B\",\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model qwen2.5 for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"qwen2.5\",\n",
    "                                          model_size=\"0.5B\",\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-de with model mistral for dataset wnt23...\n",
      "Total number of samples: 557; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/SNLP-project/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 2/2 [00:22<00:00, 11.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating de-en with model mistral for dataset wnt23...\n",
      "Total number of samples: 549; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-cs with model mistral for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:39<00:00, 19.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating cs-en with model mistral for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-is with model mistral for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:54<00:00, 27.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating is-en with model mistral for dataset wnt23...\n",
      "Total number of samples: 1000; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-zh with model mistral for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating zh-en with model mistral for dataset wnt23...\n",
      "Total number of samples: 1976; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating en-ru with model mistral for dataset wnt23...\n",
      "Total number of samples: 2074; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:25<00:00, 42.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating ru-en with model mistral for dataset wnt23...\n",
      "Total number of samples: 1723; reduced to 2 (numpy seed = 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_translation_different_directions(directions,\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"mistral\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_translation_different_directions(directions, dataset_name, model_name, batch_size):\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "\n",
    "    # Loading corresponding model\n",
    "    tokenizer, model, get_input_targets_fn, tslt_fn = load_model(model_name)\n",
    "\n",
    "    evaluation_all_direction = {}\n",
    "    for direction in directions:\n",
    "        input_language, target_language = direction[0:2], direction[3:5]\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "        # Extracting input & targets\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        translation_pred = tslt_fn(inputs, model, tokenizer, batch_size, target_language)\n",
    "        with open(f\"./generated_translations/evaluations/{dataset_name}_{model_name}_{direction}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(translation_pred, f, pickle.HIGHEST_PROTOCOL)\n",
    "        evaluation_all_direction[f\"{direction}\"] = evaluate_translation(sources, targets, translation_pred)\n",
    "    return evaluation_all_direction\n",
    "\n",
    "def eval_translation_general(directions, dataset_names, model_names, batch_size):\n",
    "    evaluation_all_datasets = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        evaluation_all_models = {}\n",
    "        for model_name in model_names:\n",
    "            evaluation_all_models[f\"{model_name}\"] = eval_translation_different_directions(directions, dataset_name, model_name, batch_size)\n",
    "        evaluation_all_datasets[f\"{dataset_name}\"] = evaluation_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP environment",
   "language": "python",
   "name": "snlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
