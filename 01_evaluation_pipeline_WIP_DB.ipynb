{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset, Dataset\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from sacrebleu.tokenizers.tokenizer_13a import Tokenizer13a\n",
    "from sacrebleu.tokenizers.tokenizer_zh import TokenizerZh\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 5\n",
    "max_new_tokens = 512\n",
    "top_p = 0.9\n",
    "temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tested = [\"en\", \"de\", \"cs\", \"is\", \"zh\", \"ru\"] # Only from or to english\n",
    "metrics_available = [\"bleu\", \"rouge\", \"bleurt\", \"sacrebleu\", \"comet\", \"meteor\", \"chrf\", \"bert_score\"]\n",
    "models_available = [\n",
    "    # NLLB\n",
    "    \"facebook/nllb-200-distilled-600M\",\n",
    "    # ALMA\n",
    "    \"haoranxu/ALMA-7B\",\n",
    "    # Llama 3 Instruct\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # Falcon 3 Mamba Instruct\n",
    "    \"tiiuae/Falcon3-Mamba-7B-Instruct\",\n",
    "    # Falcon 3 Instruct\n",
    "    \"tiiuae/Falcon3-1B-Instruct\",\n",
    "    \"tiiuae/Falcon3-3B-Instruct\",\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "    # Qwen 2.5 Mamba Instruct\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    # Mistral Instruct\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    # BayLing\n",
    "    \"ICTNLP/bayling-2-7b\",\n",
    "    # Bloom & Bloomz\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloom-3b\",\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    "    \"bigscience/bloomz-3b\",\n",
    "    \"bigscience/bloomz-7b1\",\n",
    "    # OPT\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"facebook/opt-iml-1.3b\",\n",
    "    # MPT\n",
    "    \"mosaicml/mpt-7b-instruct\",\n",
    "]\n",
    "\n",
    "\n",
    "ds_available = [\"haoranxu/WMT23-Test\",\n",
    "                \"openlanguagedata/flores_plus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################   NLLB\n",
    "\n",
    "def get_input_targets_NLLB(dataset_wnt_format, source_lang, target_lang):\n",
    "    inputs = [example[source_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    return inputs, inputs, targets\n",
    "\n",
    "def translate_list_of_str_NLLB(list_str, tokenizer, model, to_laguage):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    equivalence_language_to_FLORES = {\"en\": \"eng_Latn\", \"de\": \"deu_Latn\", \"ru\": \"rus_Cyrl\", \"is\": \"isl_Latn\", \"zh\": \"zho_Hans\", \"cs\": \"ces_Latn\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        language_tgt_FLORES = equivalence_language_to_FLORES[to_laguage]\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(language_tgt_FLORES),\n",
    "                                    num_beams=num_beams, max_length=max_new_tokens, early_stopping=True,\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_NLLB(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size = 4\n",
    "    For 16GB VRAM, use batch_size = 8 (better working with unbatch version to avoid pad noise).\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_NLLB(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   ALMA\n",
    "\n",
    "def get_input_targets_ALMA(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_ALMA(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_ALMA(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=1\n",
    "    For 16GB VRAM, use batch_size=3\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_ALMA(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Llama 3\n",
    "\n",
    "def get_input_targets_Llama3(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "        \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Llama3(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|end_of_text|>\")[0]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>assistant\\n\")[-1]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>\")[-1]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Llama3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\")\n",
    "        out_tokens = model.generate(**tokens.to(device),\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Llama3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Llama3(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM use\n",
    "        batch_size=20 with Llama3 1B,\n",
    "        batch_size=4 with Llama3 3B\n",
    "    For 16 GB VRAM use \n",
    "        batch_size=40 with Llama3 1B,\n",
    "        batch_size=10 with Llama3 3B,\n",
    "        batch_size=5 with Llama3 8B,\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Llama3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language=None)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Falcon 3 (Normal + Mamba)\n",
    "\n",
    "def get_input_targets_Falcon3(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Falcon3Mamba(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|im_end|>\\n<|im_start|>assistant\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|im_end|>\")[0]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Falcon3Mamba(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3Mamba(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3Mamba(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=4 with Falcon Mamba 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon Mamba 7B (4 bits quantization),\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3Mamba(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "def extract_translation_Falcon3(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|assistant|>\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|pad|>\")[-1]\n",
    "    translation_only = answer.split(\"<|endoftext|>\")[0]\n",
    "    translation_only = re.sub(r\"^[^a-zA-Z0-9]*\", \"\", translation_only)\n",
    "    return translation_only.replace(\"assistant|>\\n\", \"\")\n",
    "\n",
    "def translate_list_of_str_Falcon3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=8 with Falcon 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon 3B,\n",
    "        batch_size=12 with Falcon 1B\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Qwen 2.5\n",
    "\n",
    "def get_input_targets_Qwen2_5(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Qwen2_5(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|im_start|>assistant\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|im_end|>\")[0]\n",
    "    translation_only = answer.replace(\"<|endoftext|>\", \"\")\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Qwen2_5(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Qwen2_5(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Qwen2_5(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=100 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Qwen2_5(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Mistral\n",
    "\n",
    "\n",
    "def get_input_targets_Mistral(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Mistral(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"[/INST] \")[-1]\n",
    "    answer = answerpadded.split(\"</s>\")[0]\n",
    "    return answer\n",
    "\n",
    "def translate_list_of_str_Mistral(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Mistral(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Mistral(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=2 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Mistral(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   BayLing\n",
    "\n",
    "def get_input_targets_BayLing(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_BayLing(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_BayLing(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_BayLing(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   BLOOM & BLOOMZ\n",
    "\n",
    "def get_input_targets_BLOOM(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_BLOOM(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_BLOOM(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_BLOOM(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   OPT & OPT Instruct\n",
    "\n",
    "def get_input_targets_OPT(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_OPT(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_OPT(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_OPT(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   MPT\n",
    "\n",
    "def get_input_targets_MPT(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use the instruct template\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n### Instruction:\"\n",
    "        + f\"Translate from {source_lang_name} to {target_lang_name}: {example.get(source_lang)}\"\n",
    "        + \"\\n### Response:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_MPT(list_str, tokenizer, model, target_language=None):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translated_text = [t.split(\"\\n### Response:\")[-1] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_MPT(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_MPT(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_flores_to_some_languages(ds_flores, directions):\n",
    "    print(\"Extracting all languages in directions from FLORES...\")\n",
    "    list_languages = []\n",
    "    for direction in directions:\n",
    "        lang1, lang2 = direction[0:2], direction[3:5]\n",
    "        if lang1 not in list_languages:\n",
    "            list_languages.append(lang1)\n",
    "        if lang2 not in list_languages:\n",
    "            list_languages.append(lang2)\n",
    "\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    ds_list = []\n",
    "    for elem in ds_flores:\n",
    "        for lang in list_languages:\n",
    "            if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "                if lang == \"zh\":\n",
    "                    if elem[\"glottocode\"] == \"beij1234\":\n",
    "                        ds_list.append(elem)\n",
    "                else:\n",
    "                    ds_list.append(elem)\n",
    "    return Dataset.from_list(ds_list)\n",
    "\n",
    "def transform_to_WNT_style(ds_flores, lang, lang_start=\"en\"):\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    list_sentence_lang, list_sentence_lang_start = [], []\n",
    "    for elem in ds_flores:\n",
    "        if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "            if lang == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang.append(elem[\"text\"])\n",
    "\n",
    "        elif elem[\"iso_639_3\"] == language_to_iso[lang_start]:\n",
    "            if lang_start == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang_start.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang_start.append(elem[\"text\"])\n",
    "    assert len(list_sentence_lang) == len(list_sentence_lang_start)\n",
    "    #print(f\"Number of samples: {len(list_sentence_lang)}\")\n",
    "    final_text_list = []\n",
    "    for i in range(len(list_sentence_lang)):\n",
    "        final_text_list.append({f\"{lang_start}\": list_sentence_lang_start[i],\n",
    "                                f\"{lang}\": list_sentence_lang[i],})\n",
    "    return Dataset.from_dict({f\"{lang_start}-{lang}\": final_text_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling\n",
    "We use WNT23 from the authors preprocessed split and the FLORES+ dataset, format in the same way that the WNT23 is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNT23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wnt = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "print(len(ds_wnt), ds_wnt[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_flores_to_some_languages(ds_flores, directions):\n",
    "    list_languages = []\n",
    "    for direction in directions:\n",
    "        lang1, lang2 = direction[0:2], direction[3:5]\n",
    "        if lang1 not in list_languages:\n",
    "            list_languages.append(lang1)\n",
    "        if lang2 not in list_languages:\n",
    "            list_languages.append(lang2)\n",
    "\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    ds_list = []\n",
    "    for elem in ds_flores:\n",
    "        for lang in list_languages:\n",
    "            if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "                if lang == \"zh\":\n",
    "                    if elem[\"glottocode\"] == \"beij1234\":\n",
    "                        ds_list.append(elem)\n",
    "                else:\n",
    "                    ds_list.append(elem)\n",
    "    return Dataset.from_list(ds_list)\n",
    "\n",
    "def transform_to_WNT_style(ds_flores, lang, lang_start=\"en\"):\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    list_sentence_lang, list_sentence_lang_start = [], []\n",
    "    for elem in ds_flores:\n",
    "        if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "            if lang == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang.append(elem[\"text\"])\n",
    "\n",
    "        elif elem[\"iso_639_3\"] == language_to_iso[lang_start]:\n",
    "            if lang_start == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang_start.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang_start.append(elem[\"text\"])\n",
    "    assert len(list_sentence_lang) == len(list_sentence_lang_start)\n",
    "    #print(f\"Number of samples: {len(list_sentence_lang)}\")\n",
    "    final_text_list = []\n",
    "    for i in range(len(list_sentence_lang)):\n",
    "        final_text_list.append({f\"{lang_start}\": list_sentence_lang_start[i],\n",
    "                                f\"{lang}\": list_sentence_lang[i],})\n",
    "    return Dataset.from_dict({f\"{lang_start}-{lang}\": final_text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "ds_flores_reduced = reduce_flores_to_some_languages(ds_flores, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "ds_flores_wnt_style = transform_to_WNT_style(ds_flores, lang=\"zh\", lang_start=\"en\")\n",
    "print(f\"Time to compute: {time.time()-t1:.2f}s\", ds_flores_wnt_style[0:4])\n",
    "\n",
    "t1 = time.time()\n",
    "ds_flores_wnt_style_reduced = transform_to_WNT_style(ds_flores_reduced, lang=\"zh\", lang_start=\"en\")\n",
    "print(f\"Time to compute: {time.time()-t1:.2f}s\", ds_flores_wnt_style_reduced[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & inference loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_NLLB(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(translate_list_of_str_NLLB(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and LoRA weights\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_ALMA(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_ALMA(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Llama3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Llama3(inputs[0:5], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3 not instructed 4B (for comparaison to finetuned version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "Q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "                                bnb_4bit_use_double_quant=False)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets_Llama3NI4bit(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name} and end your answer as soon as the task is finished:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_Llama3NI4bit(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(**inputs.to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[-1] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_Llama3NI4bit(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Llama3NI4bit(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Llama3NI4bit(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_Llama3NI4bit(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon 3 Instruct (mamba and transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Falcon3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Falcon3Mamba(inputs[0:4], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Falcon3(inputs[0:8], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Qwen2_5(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(translate_list_of_str_Qwen2_5(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Mistral(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_Mistral(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"ICTNLP/bayling-2-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"ICTNLP/bayling-2-7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_BayLing(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_BayLing(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom & Bloom Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b7\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1\")\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_BLOOM(ds_wnt, \"en\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_batched_BLOOM(inputs[0:2], model, tokenizer, batch_size=1, target_language=\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPT & OPT Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-iml-1.3b\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-1.3b\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_OPT(ds_wnt, \"en\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_batched_OPT(inputs[4:6], model, tokenizer, batch_size=1, target_language=\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_MPT(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,6):\n",
    "    print(translate_list_of_str_MPT(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General aggregation functions for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_size=None):\n",
    "    if model_name == \"alma\":\n",
    "        tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        \n",
    "    elif model_name == \"nllb\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "            nQ_cofig = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    elif model_name == \"llama3-NI-4bit\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "        nQ_cofig = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                      bnb_4bit_quant_type=\"nf4\",\n",
    "                                      bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "                                      bnb_4bit_use_double_quant=False)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    elif model_name == \"falcon3-mamba\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"falcon3\":\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    elif model_name == \"qwen2.5\":\n",
    "        if model_size==\"0.5B\" or model_size==\"1.5B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"mistral\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    elif model_name == \"bayling\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"ICTNLP/bayling-2-7b\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"ICTNLP/bayling-2-7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"bloom\":\n",
    "        if model_size==\"0.5B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "        elif model_size==\"1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "\n",
    "    elif model_name == \"bloomz\":\n",
    "        if model_size==\"1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b7\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"opt\":\n",
    "        if model_size==\"0.1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"0.3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"opt-instruct\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-iml-1.3b\")\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-1.3b\", torch_dtype=\"auto\", device_map=device)\n",
    "    \n",
    "    elif model_name == \"mpt\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    return tokenizer, model\n",
    "\n",
    "def get_support_fn(model_name):\n",
    "    if model_name == \"alma\":\n",
    "        get_input_targets_fn = get_input_targets_ALMA\n",
    "        tslt_fn = translate_batched_ALMA\n",
    "        \n",
    "    elif model_name == \"nllb\":\n",
    "        get_input_targets_fn = get_input_targets_NLLB\n",
    "        tslt_fn = translate_batched_NLLB\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "        get_input_targets_fn = get_input_targets_Llama3\n",
    "        tslt_fn = translate_batched_Llama3\n",
    "    \n",
    "    elif model_name == \"llama3-NI-4bit\":\n",
    "        get_input_targets_fn = get_input_targets_Llama3NI4bit\n",
    "        tslt_fn = translate_batched_Llama3NI4bit\n",
    "    \n",
    "    elif model_name == \"falcon3-mamba\":\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3Mamba\n",
    "    \n",
    "    elif model_name == \"falcon3\":\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3\n",
    "    \n",
    "    elif model_name == \"qwen2.5\":\n",
    "        get_input_targets_fn = get_input_targets_Qwen2_5\n",
    "        tslt_fn = translate_batched_Qwen2_5\n",
    "    \n",
    "    elif model_name == \"mistral\":\n",
    "        get_input_targets_fn = get_input_targets_Mistral\n",
    "        tslt_fn = translate_batched_Mistral\n",
    "    \n",
    "    elif model_name == \"bayling\":\n",
    "        get_input_targets_fn = get_input_targets_BayLing\n",
    "        tslt_fn = translate_batched_BayLing\n",
    "\n",
    "    elif model_name == \"bloom\" or model_name == \"bloomz\":\n",
    "        get_input_targets_fn = get_input_targets_BLOOM\n",
    "        tslt_fn = translate_batched_BLOOM\n",
    "    \n",
    "    elif model_name == \"opt\" or model_name == \"opt-instruct\":\n",
    "        get_input_targets_fn = get_input_targets_OPT\n",
    "        tslt_fn = translate_batched_OPT\n",
    "    \n",
    "    elif model_name == \"mpt\":\n",
    "        get_input_targets_fn = get_input_targets_MPT\n",
    "        tslt_fn = translate_batched_MPT\n",
    "        \n",
    "    return get_input_targets_fn, tslt_fn\n",
    "\n",
    "def get_inp_tgt_lang(direction):\n",
    "    return direction[0:2], direction[3:5]\n",
    "\n",
    "def reduce_dataset(inputs, sources, targets, final_nb):\n",
    "    idx = np.arange(len(inputs))\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(idx, final_nb)\n",
    "    return [inputs[i] for i in idx], [sources[i] for i in idx], [targets[i] for i in idx]\n",
    "\n",
    "def get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size):\n",
    "    mod_size = \"-\"+model_size if model_size is not None else \"\"\n",
    "    return f\"./generated_translations/evaluations/{dataset_name}_{model_name}{mod_size}_{direction}_red-{reduce_size}.pkl\"\n",
    "\n",
    "def get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size):\n",
    "    mod_size = \"-\"+model_size if model_size is not None else \"\"\n",
    "    return f\"./evaluations/raw_{dataset_name}_{model_name}{mod_size}_{direction}_red-{reduce_size}.pkl\"\n",
    "\n",
    "\n",
    "def generate_translation_different_directions(directions, dataset_name, model_name, batch_size, reduce_size = None, model_size = None):\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "\n",
    "    # Loading corresponding model\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer, model = load_model(model_name, model_size)\n",
    "    get_input_targets_fn, tslt_fn = get_support_fn(model_name)\n",
    "\n",
    "    for direction in directions:\n",
    "        print(f\"Translating {direction} with model {model_name}\"\n",
    "              +(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "              +f\" for dataset {dataset_name}...\")\n",
    "        input_language, target_language = get_inp_tgt_lang(direction)\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            if direction != \"cs-en\":\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "            else:\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "                ds = Dataset.from_dict({f\"cs-en\": ds[\"en-cs\"][::-1]}) # Reverse list to avoid having same sentences (if reduce_size not None)\n",
    "        # Extracting input & targets\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        print(f\"Total number of samples: {len(sources)}\" + (\"\" if reduce_size is None else f\"; reduced to {reduce_size} (numpy seed = 42)\"))\n",
    "        if reduce_size is not None:\n",
    "            sources, inputs, targets = reduce_dataset(sources, inputs, targets, reduce_size)\n",
    "        translation_pred = tslt_fn(inputs, model, tokenizer, batch_size, target_language)\n",
    "        if not os.path.exists(f\"./generated_translations/evaluations\"):\n",
    "            os.makedirs(f\"./generated_translations/evaluations\")\n",
    "        translations_filename = get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "        with open(translations_filename, \"wb\") as f:\n",
    "            pickle.dump(translation_pred, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # De-load model from GPU to enable calling this function with another model without restarting kernel\n",
    "    model.cpu()\n",
    "    del model, tokenizer\n",
    "\n",
    "def generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size):\n",
    "    for model_name, model_size in zip(model_names, model_sizes):\n",
    "        generate_translation_different_directions(directions=directions,\n",
    "                                                dataset_name=dataset_name,\n",
    "                                                model_name=model_name,\n",
    "                                                model_size=model_size,\n",
    "                                                batch_size=batch_size,\n",
    "                                                reduce_size=reduce_size)\n",
    "        \n",
    "def generate_translation_several_datasets(directions, dataset_names, model_names, model_sizes, batch_size, reduce_size):\n",
    "    for dataset_name in dataset_names:\n",
    "        generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\"]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"qwen2.5\",\n",
    "               \"mistral\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"bloom\", \"bloom\"]\n",
    "model_sizes = [\"0.5B\", \"1B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\"]\n",
    "model_names = [\"bloom\"]\n",
    "model_sizes = [\"7B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"bloomz\"]\n",
    "model_sizes = [\"7B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"opt-instruct\"]\n",
    "model_sizes = [None]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"bloomz\",\n",
    "               \"opt-instruct\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"flores\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Left to do if time ###\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"opt\",\n",
    "               \"mpt\",\n",
    "               \"bayling\",\n",
    "               \"bloomz\", \"bloomz\",\n",
    "               \"bloom\", \"bloom\", \"bloom\", \"bloom\",\n",
    "               \"opt\", \"opt\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None,\n",
    "               None,\n",
    "               \"1B\", \"3B\",\n",
    "               \"0.5B\", \"1B\", \"3B\", \"7B\",\n",
    "               \"0.1B\", \"0.3B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"flores\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=200)\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"opt\",\n",
    "               \"mpt\",\n",
    "               \"bayling\",\n",
    "               \"bloomz\", \"bloomz\",\n",
    "               \"bloom\",\n",
    "               \"opt\", \"opt\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None,\n",
    "               None,\n",
    "               \"1B\", \"3B\",\n",
    "               \"3B\",\n",
    "               \"0.1B\", \"0.3B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)\n",
    "\n",
    "directions = [\"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"bloom\"]\n",
    "model_sizes = [\"7B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\"]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None]\n",
    "               \n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"flores\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_translation_different_directions(directions=[\"en-de\", \"de-en\",\n",
    "                                                        \"en-cs\", \"cs-en\"],\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"falcon3-mamba\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_translation_different_directions(directions=[\"en-is\", \"is-en\",\n",
    "                                                        \"en-zh\", \"zh-en\",\n",
    "                                                        \"en-ru\", \"ru-en\"],\n",
    "                                          dataset_name=\"wnt23\",\n",
    "                                          model_name=\"falcon3-mamba\",\n",
    "                                          model_size=None,\n",
    "                                          batch_size=1,\n",
    "                                          reduce_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from predictions: evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rouge(metric, sources, targets, translation_infered, target_language):\n",
    "    out_rouge = metric.compute(predictions=translation_infered,\n",
    "                                  references=targets,\n",
    "                                  use_aggregator=False)\n",
    "    # For further statistical treatment\n",
    "    results_rouge = {\"rouge1\": {},\n",
    "                     \"rouge2\": {},\n",
    "                     \"rougeL\": {},\n",
    "                     \"rougeLsum\": {},}\n",
    "    for key in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "        results_rouge[key][\"mean_score\"] = np.mean(out_rouge[key]).item()\n",
    "        results_rouge[key][\"std_score\"] = np.std(out_rouge[key]).item()\n",
    "        results_rouge[key][\"std_unbias_score\"] = np.std(out_rouge[key], ddof=1).item()\n",
    "    return results_rouge\n",
    "\n",
    "def eval_bleu(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bleu = {\"scores\": [], \"brevity_penalty\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            bleu_out = metric.compute(predictions=[trans],\n",
    "                                    references=[[tgt]],\n",
    "                                    tokenizer = TokenizerZh() if target_language==\"zh\" else Tokenizer13a())\n",
    "        except ZeroDivisionError:\n",
    "            bleu_out={\"bleu\": 0., \"brevity_penalty\": 0.}\n",
    "\n",
    "        results_bleu[\"scores\"].append(bleu_out[\"bleu\"])\n",
    "        results_bleu[\"brevity_penalty\"].append(bleu_out[\"brevity_penalty\"])\n",
    "    # For further statistical treatment\n",
    "    results_bleu[\"mean_score\"] = np.mean(results_bleu[\"scores\"]).item()\n",
    "    results_bleu[\"std_score\"] = np.std(results_bleu[\"scores\"]).item()\n",
    "    results_bleu[\"std_unbias_score\"] = np.std(results_bleu[\"scores\"], ddof=1).item()\n",
    "    return {\"bleu\": results_bleu}\n",
    "\n",
    "def eval_sacrebleu(metric, sources, targets, translation_infered, target_language):\n",
    "    results_sacrebleu = {\"scores\": [], \"brevity_penalty\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            sacrebleu_out = metric.compute(predictions=[trans],\n",
    "                                            references=[[tgt]],\n",
    "                                            tokenize = \"zh\" if target_language==\"zh\" else \"13a\")\n",
    "        except ZeroDivisionError:\n",
    "            sacrebleu_out = {\"score\": 0., \"bp\": 0.}\n",
    "        results_sacrebleu[\"scores\"].append(sacrebleu_out[\"score\"])\n",
    "        results_sacrebleu[\"brevity_penalty\"].append(sacrebleu_out[\"bp\"])\n",
    "    # For further statistical treatment\n",
    "    results_sacrebleu[\"mean_score\"] = np.mean(results_sacrebleu[\"scores\"]).item()\n",
    "    results_sacrebleu[\"std_score\"] = np.std(results_sacrebleu[\"scores\"]).item()\n",
    "    results_sacrebleu[\"std_unbias_score\"] = np.std(results_sacrebleu[\"scores\"], ddof=1).item()\n",
    "    return {\"sacrebleu\": results_sacrebleu}\n",
    "\n",
    "def eval_chrf_and_chrfplusplus(metric, sources, targets, translation_infered, target_language):\n",
    "    results_chrf = {\"scores\": []}\n",
    "    results_chrfplusplus = {\"scores\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            chrf_out = metric.compute(predictions=[trans],\n",
    "                                    references=[[tgt]],\n",
    "                                    word_order=0,\n",
    "                                    eps_smoothing=False)\n",
    "        except ZeroDivisionError:\n",
    "            chrf_out = {\"score\": 0.}\n",
    "        try:\n",
    "            chrfplusplus_out = metric.compute(predictions=[trans],\n",
    "                                            references=[[tgt]],\n",
    "                                            word_order=2,\n",
    "                                            eps_smoothing=True)\n",
    "        except ZeroDivisionError:\n",
    "            chrfplusplus_out = {\"score\": 0.}\n",
    "        results_chrf[\"scores\"].append(chrf_out['score'])\n",
    "        results_chrfplusplus[\"scores\"].append(chrfplusplus_out['score'])\n",
    "    # For further statistical treatment\n",
    "    results_chrf[\"mean_score\"] = np.mean(results_chrf[\"scores\"]).item()\n",
    "    results_chrf[\"std_score\"] = np.std(results_chrf[\"scores\"]).item()\n",
    "    results_chrf[\"std_unbias_score\"] = np.std(results_chrf[\"scores\"], ddof=1).item()\n",
    "    results_chrfplusplus[\"mean_score\"] = np.mean(results_chrfplusplus[\"scores\"]).item()\n",
    "    results_chrfplusplus[\"std_score\"] = np.std(results_chrfplusplus[\"scores\"]).item()\n",
    "    results_chrfplusplus[\"std_unbias_score\"] = np.std(results_chrfplusplus[\"scores\"], ddof=1).item()\n",
    "    return {\"chrf\": results_chrf,\n",
    "            \"chrfplusplus\": results_chrfplusplus}\n",
    "\n",
    "def eval_comet(metric, sources, targets, translation_infered, target_language):\n",
    "    results_comet = metric.compute(predictions=translation_infered,\n",
    "                                         references=targets,\n",
    "                                         sources=sources)\n",
    "    # For further statistical treatment\n",
    "    results_comet.update({\"std_score\": np.std(results_comet[\"scores\"]).item(),\n",
    "                          \"std_unbias_score\": np.std(results_comet[\"scores\"], ddof=1).item()})\n",
    "    return {\"comet\": results_comet}\n",
    "\n",
    "def eval_bleurt(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bleurt = metric.compute(predictions=translation_infered,\n",
    "                                    references=targets)\n",
    "    # For further statistical treatment\n",
    "    results_bleurt.update({\"mean_score\": np.mean(results_bleurt[\"scores\"]).item(),\n",
    "                           \"std_score\": np.std(results_bleurt[\"scores\"]).item(),\n",
    "                           \"std_unbias_score\": np.std(results_bleurt[\"scores\"], ddof=1).item()})\n",
    "    return {\"bleurt\": results_bleurt}\n",
    "\n",
    "def eval_bertscore(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bert = metric.compute(predictions=translation_infered, references=targets, lang=target_language)\n",
    "    # For further statistical treatment\n",
    "    results_bert.update({\"mean_score\": np.mean(results_bert[\"f1\"]).item(),\n",
    "                         \"std_score\": np.std(results_bert[\"f1\"]).item(),\n",
    "                         \"std_unbias_score\": np.std(results_bert[\"f1\"], ddof=1).item()})\n",
    "    return {\"bertscore\": results_bert}\n",
    "\n",
    "def eval_meteor(metric, sources, targets, translation_infered, target_language):\n",
    "    results_meteor = {\"scores\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        meteor_out = metric.compute(predictions=[trans],\n",
    "                                    references=[tgt])\n",
    "        results_meteor[\"scores\"].append(meteor_out[\"meteor\"])\n",
    "    # For further statistical treatment\n",
    "    results_meteor[\"mean_score\"] = np.mean(results_meteor[\"scores\"]).item()\n",
    "    results_meteor[\"std_score\"] = np.std(results_meteor[\"scores\"]).item()\n",
    "    results_meteor[\"std_unbias_score\"] = np.std(results_meteor[\"scores\"], ddof=1).item()\n",
    "    return {\"meteor\": results_meteor}\n",
    "\n",
    "def get_eval_fn(metric_name):\n",
    "    if metric_name == \"rouge\":\n",
    "        return eval_rouge\n",
    "    elif metric_name == \"bleu\":\n",
    "        return eval_bleu\n",
    "    elif metric_name == \"sacrebleu\":\n",
    "        return eval_sacrebleu\n",
    "    elif metric_name == \"chrf\":\n",
    "        return eval_chrf_and_chrfplusplus\n",
    "    elif metric_name == \"comet\":\n",
    "        return eval_comet\n",
    "    elif metric_name == \"bleurt\":\n",
    "        return eval_bleurt\n",
    "    elif metric_name == \"bertscore\":\n",
    "        return eval_bertscore\n",
    "    elif metric_name == \"meteor\":\n",
    "        return eval_meteor\n",
    "\n",
    "def load_metric(metric_name):\n",
    "    if metric_name == \"rouge\":\n",
    "        return evaluate.load('rouge')\n",
    "    elif metric_name == \"bleu\":\n",
    "        return evaluate.load(\"bleu\")\n",
    "    elif metric_name == \"sacrebleu\":\n",
    "        return evaluate.load(\"sacrebleu\")\n",
    "    elif metric_name == \"chrf\":\n",
    "        return evaluate.load(\"chrf\")\n",
    "    elif metric_name == \"comet\":\n",
    "        return evaluate.load('comet')\n",
    "    elif metric_name == \"bleurt\":\n",
    "        return evaluate.load('bleurt', 'bleurt-large-512')\n",
    "    elif metric_name == \"bertscore\":\n",
    "        return evaluate.load(\"bertscore\")\n",
    "    elif metric_name == \"meteor\":\n",
    "        return evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_metric_one_model(metric_name, metric, directions, dataset_name, model_name, model_size, reduce_size):\n",
    "    # Getting right evaluation function\n",
    "    metric_eval_fn = get_eval_fn(metric_name)\n",
    "\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "        ds_flores = reduce_flores_to_some_languages(ds_flores, directions)\n",
    "\n",
    "    for direction in directions:\n",
    "        print(f\"Evaluating translations {direction} with model {model_name}\"\n",
    "              +(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "              +f\" for dataset {dataset_name}...\")\n",
    "        input_language, target_language = get_inp_tgt_lang(direction)\n",
    "\n",
    "        # Loading previous eval if existing\n",
    "        eval_filename = get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "        if not os.path.exists(f\"./evaluations\"):\n",
    "            os.makedirs(f\"./evaluations\")\n",
    "        if os.path.exists(eval_filename):\n",
    "            with open(eval_filename, \"rb\") as f:\n",
    "                complete_eval = pickle.load(f)\n",
    "        else:\n",
    "            complete_eval = {}\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            if direction != \"cs-en\":\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "            else:\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "                ds = Dataset.from_dict({f\"cs-en\": ds[\"en-cs\"][::-1]}) # Reverse list to avoid having same sentences (if reduce_size not None)\n",
    "        \n",
    "        # Extracting input & targets\n",
    "        get_input_targets_fn, _ = get_support_fn(model_name)\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        print(f\"Total number of samples: {len(sources)}\" + (\"\" if reduce_size is None else f\"; reduced to {reduce_size} (numpy seed = 42)\"))\n",
    "        if reduce_size is not None:\n",
    "            # /!\\ Use same reduce size and same seed to ensure sources and previous inputs are the same /!\\\n",
    "            sources, inputs, targets = reduce_dataset(sources, inputs, targets, reduce_size)\n",
    "\n",
    "        # Loading precomputed translations\n",
    "        translations_filename = get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "        with open(translations_filename, \"rb\") as f:\n",
    "            translation_pred = pickle.load(f)\n",
    "        \n",
    "        # Evaluation translation for this direction\n",
    "        eval_dict = metric_eval_fn(metric, sources, targets, translation_pred, target_language)\n",
    "        complete_eval.update(eval_dict)\n",
    "\n",
    "        with open(eval_filename, \"wb\") as f:\n",
    "            pickle.dump(complete_eval, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def eval_one_metric(metric_name, directions, dataset_names, model_names, model_sizes, reduce_sizes):\n",
    "    print(f\"Computing evaluations with {metric_name}...\")\n",
    "    metric = load_metric(metric_name)\n",
    "    for dataset_name, reduce_size in zip(dataset_names, reduce_sizes):\n",
    "        for model_name, model_size in zip(model_names, model_sizes):\n",
    "            eval_one_metric_one_model(metric_name, metric, directions, dataset_name, model_name, model_size, reduce_size)\n",
    "\n",
    "def eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes):\n",
    "    for metric_name in metric_names:\n",
    "        eval_one_metric(metric_name, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"rouge\", \"bleu\", \"sacrebleu\", \"chrf\", \"comet\", \"meteor\", \"bertscore\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"bleurt\"]\n",
    "\n",
    "dataset_names = [\"flores\", \"wnt23\"]\n",
    "reduce_sizes = [200, 100]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"bloomz\",\n",
    "               \"opt-instruct\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelCoordinatesPlot(title, N, data, category, ynames, colors=None, category_names=None, savepath=None):\n",
    "    \"\"\"\n",
    "    A legend is added, if category_names is not None.\n",
    "\n",
    "    :param title: The title of the plot.\n",
    "    :param N: Number of data sets (i.e., lines).\n",
    "    :param data: A list containing one array per parallel axis, each containing N data points.\n",
    "    :param category: An array containing the category of each data set.\n",
    "    :param category_names: Labels of the categories. Must have the same length as set(category).\n",
    "    :param ynames: The labels of the parallel axes.\n",
    "    :param colors: A colormap to use.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, host = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "    # organize the data\n",
    "    ys = np.dstack(data)[0]\n",
    "    ymins = ys.min(axis=0)\n",
    "    ymaxs = ys.max(axis=0)\n",
    "    dys = ymaxs - ymins\n",
    "    ymins -= dys * 0.05  # add 5% padding below and above\n",
    "    ymaxs += dys * 0.05\n",
    "    dys = ymaxs - ymins\n",
    "\n",
    "    # transform all data to be compatible with the main axis\n",
    "    zs = np.zeros_like(ys)\n",
    "    zs[:, 0] = ys[:, 0]\n",
    "    zs[:, 1:] = (ys[:, 1:] - ymins[1:]) / dys[1:] * dys[0] + ymins[0]\n",
    "\n",
    "    axes = [host] + [host.twinx() for i in range(ys.shape[1] - 1)]\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_ylim(ymins[i], ymaxs[i])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        if ax != host:\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "            ax.spines[\"right\"].set_position((\"axes\", i / (ys.shape[1] - 1)))\n",
    "\n",
    "    host.set_xlim(0, ys.shape[1] - 1)\n",
    "    host.set_xticks(range(ys.shape[1]))\n",
    "    host.set_xticklabels(ynames, fontsize=7)\n",
    "    host.tick_params(axis='x', which='major', pad=7)\n",
    "    host.spines['right'].set_visible(False)\n",
    "    host.xaxis.tick_top()\n",
    "    host.set_title(title, fontsize=15)\n",
    "\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors\n",
    "    if category_names is not None:\n",
    "        legend_handles = [None for _ in category_names]\n",
    "    else:\n",
    "        legend_handles = [None for _ in set(category)]\n",
    "    for j in range(N):\n",
    "        # to just draw straight lines between the axes:\n",
    "        # host.plot(range(ys.shape[1]), zs[j,:], c=colors[(category[j] - 1) % len(colors) ])\n",
    "\n",
    "        # create bezier curves\n",
    "        # for each axis, there will a control vertex at the point itself, one at 1/3rd towards the previous and one\n",
    "        #   at one third towards the next axis; the first and last axis have one less control vertex\n",
    "        # x-coordinate of the control vertices: at each integer (for the axes) and two inbetween\n",
    "        # y-coordinate: repeat every point three times, except the first and last only twice\n",
    "        verts = list(zip([x for x in np.linspace(0, len(ys) - 1, len(ys) * 3 - 2, endpoint=True)],\n",
    "                         np.repeat(zs[j, :], 3)[1:-1]))\n",
    "        # for x,y in verts: host.plot(x, y, 'go') # to show the control points of the beziers\n",
    "        codes = [Path.MOVETO] + [Path.CURVE4 for _ in range(len(verts) - 1)]\n",
    "        path = Path(verts, codes)\n",
    "        patch = patches.PathPatch(path, facecolor='none', lw=1, edgecolor=colors[category[j]])\n",
    "        legend_handles[category[j]] = patch\n",
    "        host.add_patch(patch)\n",
    "\n",
    "        if category_names is not None:\n",
    "            host.legend(legend_handles, category_names,\n",
    "                        loc='lower center', bbox_to_anchor=(0.5, -0.18),\n",
    "                        ncol=len(category_names)//2, fancybox=True, shadow=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model_name(model_name, model_size):\n",
    "    return f\"{model_name}\"+(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "\n",
    "def concatenate_results(directions, models, model_sizes, datasets, reduce_sizes, metrics_names, agg_keys, verbose=False):\n",
    "    \"\"\"\n",
    "    agg_keys should be a list containing keys present in output dictonnary for every metrics desired\n",
    "    for all metrics, can be only [\"mean_score\", \"std_score\", \"std_unbias_score\"] (or less)\n",
    "    \"\"\"\n",
    "    metrics_names2metrics = {\"ROUGE-1\": \"rouge1\",\n",
    "                             \"ROUGE-2\": \"rouge2\",\n",
    "                             \"ROUGE-L\": \"rougeL\",\n",
    "                             \"ROUGE-Lsum\": \"rougeLsum\",\n",
    "                             \"BLEU\": \"bleu\",\n",
    "                             \"SacreBLEU\": \"sacrebleu\",\n",
    "                             \"chrF\": \"chrf\",\n",
    "                             \"chrF++\": \"chrfplusplus\",\n",
    "                             \"COMET\": \"comet\",\n",
    "                             \"BLEURT\": \"bleurt\",\n",
    "                             \"BERTscore\": \"bertscore\",\n",
    "                             \"METEOR\": \"meteor\"}\n",
    "    metrics = [metrics_names2metrics[name] for name in metrics_names] # Want something ordered, don't only take dico.values()\n",
    "    \n",
    "    data = {key: [[] for _ in range(len(metrics))] for key in agg_keys}\n",
    "    \n",
    "    print(\"Extracting and concatenating metrics...\")\n",
    "    for dataset_name, reduce_size in zip(datasets, reduce_sizes):\n",
    "        for model_name, model_size in zip(models, model_sizes):\n",
    "            for direction in directions:\n",
    "                eval_filename = get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "                if verbose:\n",
    "                    print(eval_filename)\n",
    "                with open(eval_filename, \"rb\") as f:\n",
    "                    evaluations = pickle.load(f)\n",
    "                for i, m in enumerate(metrics):\n",
    "                    for key in agg_keys:\n",
    "                        data[key][i].append(evaluations[m][key])\n",
    "    return data\n",
    "\n",
    "def make_parallel_plot(directions,\n",
    "                       models, model_sizes,\n",
    "                       datasets, reduce_sizes,\n",
    "                       metrics_names,\n",
    "                       list_colors_per, colors=None, verbose=False, savepath=None):\n",
    "    # Aggregate eval data\n",
    "    data = concatenate_results(directions, models, model_sizes, datasets, reduce_sizes, metrics_names, agg_keys=[\"mean_score\"], verbose=verbose)\n",
    "    data = data[\"mean_score\"]\n",
    "\n",
    "    # Generate plot categories\n",
    "    for colors_per in list_colors_per:\n",
    "        print(f\"Generating categories based {colors_per} type ('colors_per' param)...\")\n",
    "        elem2cat = ({dataset_name: i for i, dataset_name in enumerate(datasets)} if colors_per == \"dataset\"\n",
    "                    else {direction: i for i, direction in enumerate(directions)} if colors_per == \"direction\"\n",
    "                    else {get_full_model_name(model_name, model_size): i for i, (model_name, model_size) in enumerate(zip(models, model_sizes))} if colors_per == \"model\"\n",
    "                    else {})\n",
    "        dataset_name2real_name = {\"wnt23\": \"WNT23\", \"flores\": \"FLORES+\"}\n",
    "        dataset_name2real_name_and_reduction = {}\n",
    "        for dataset_name, reduce_size in zip(datasets, reduce_sizes):\n",
    "            dataset_name2real_name_and_reduction[dataset_name] = dataset_name2real_name[dataset_name] + f\" - reduct to {reduce_size} samples\"\n",
    "        category_names = ([dataset_name2real_name_and_reduction[dataset_name] for dataset_name in datasets] if colors_per == \"dataset\"\n",
    "                        else directions if colors_per == \"direction\"\n",
    "                        else [get_full_model_name(model_name, model_size) for model_name, model_size in zip(models, model_sizes)] if colors_per == \"model\"\n",
    "                        else [\"No category\"])\n",
    "        category = []\n",
    "        for dataset_name in datasets:\n",
    "            for model_name, model_size in zip(models, model_sizes):\n",
    "                for direction in directions:\n",
    "                    if colors_per == \"dataset\":\n",
    "                        category.append(elem2cat[dataset_name])\n",
    "                    elif colors_per == \"direction\":\n",
    "                        category.append(elem2cat[direction])\n",
    "                    elif colors_per == \"model\":\n",
    "                        category.append(elem2cat[get_full_model_name(model_name, model_size)])\n",
    "                    else:\n",
    "                        category.append(0)\n",
    "\n",
    "        if colors is None:\n",
    "            if colors_per == \"dataset\":\n",
    "                colors = plt.cm.Accent.colors\n",
    "            elif colors_per == \"direction\":\n",
    "                colors = plt.cm.tab20.colors\n",
    "            else:\n",
    "                colors = plt.cm.Dark2.colors + plt.cm.tab10.colors[0:7] + plt.cm.tab10.colors[8:]\n",
    "\n",
    "        # Plot\n",
    "        print(\"Plotting in parallel coordinates plot...\")\n",
    "        n_datasets, n_directions, n_models = len(directions), len(models), len(datasets)\n",
    "        parallelCoordinatesPlot(title = f\"Influence of {colors_per} on translation performances\",\n",
    "                                N = n_datasets*n_directions*n_models,\n",
    "                                data = data,\n",
    "                                category = category,\n",
    "                                category_names = category_names,\n",
    "                                ynames = metrics_names,\n",
    "                                colors=colors,\n",
    "                                savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"dataset\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/all_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\"]\n",
    "reduce_sizes = [200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\", \"nllb\", \"llama3\", \"falcon3-mamba\", \"falcon3\", \"qwen2.5\",\n",
    "               \"mistral\", \"bloomz\"]\n",
    "model_sizes = [None, None, \"8B\", None, \"7B\", \"7B\",\n",
    "               None, \"7B\", None,]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    colors=None,\n",
    "                    savepath=\"./results/evaluations_figures/main_model_flores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\"]\n",
    "reduce_sizes = [200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"llama3\",\n",
    "               \"falcon3\",\n",
    "               \"qwen2.5\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               \"3B\",\n",
    "               \"3B\",\n",
    "               \"3B\",\n",
    "               None,]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/medium_and_alma_model_flores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\"]\n",
    "reduce_sizes = [200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"llama3\",\n",
    "               \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\"]\n",
    "\n",
    "model_sizes = [None,\n",
    "               \"1B\",\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\"]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/small_and_alma_model_flores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\"]\n",
    "reduce_sizes = [200]\n",
    "\n",
    "directions = [\"en-zh\", \"zh-en\"]\n",
    "\n",
    "model_names = [\"alma\", \"nllb\",\n",
    "               \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\"]\n",
    "model_sizes = [None, None,\n",
    "               \"8B\",\n",
    "            None,\n",
    "            \"7B\",\n",
    "               \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\"]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    colors = None,\n",
    "                    savepath = \"./results/evaluations_figures/chinese_dir_model_flores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\",\n",
    "              \"en-cs\",\n",
    "              \"en-is\",\n",
    "              \"en-zh\",\n",
    "              \"en-ru\",]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\"]\n",
    "\n",
    "model_sizes = [None,\n",
    "               None]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\"],\n",
    "                    colors = plt.cm.Dark2.colors,\n",
    "                    savepath = \"./results/evaluations_figures/alma-nllb_from_en_direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"de-en\",\n",
    "              \"cs-en\",\n",
    "              \"is-en\",\n",
    "              \"zh-en\",\n",
    "              \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\"]\n",
    "\n",
    "model_sizes = [None,\n",
    "               None]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\"],\n",
    "                    colors = plt.cm.Dark2.colors,\n",
    "                    savepath = \"./results/evaluations_figures/alma-nllb_to_en_direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
