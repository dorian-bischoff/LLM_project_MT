{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    generate_translation_several_datasets,\n",
    "    generate_translation_different_directions,\n",
    "    generate_translation_several_models,\n",
    "    load_model_benchmark,\n",
    "    eval_metrics,\n",
    "    make_parallel_plot,\n",
    "    make_bar_plot,\n",
    "    make_bar_plot_all_metrics\n",
    ")\n",
    "\n",
    "from utils.eval_params import num_beams, temperature, max_new_tokens, top_p\n",
    "#num_beams = 5\n",
    "#max_new_tokens = 512\n",
    "#top_p = 0.9\n",
    "#temperature = 0.6\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tested = [\"en\", \"de\", \"cs\", \"is\", \"zh\", \"ru\"] # Only from or to english\n",
    "metrics_available = [\"bleu\", \"rouge\", \"bleurt\", \"sacrebleu\", \"comet\", \"meteor\", \"chrf\", \"bert_score\"]\n",
    "models_available = [\n",
    "    # NLLB\n",
    "    \"facebook/nllb-200-distilled-600M\",\n",
    "    # ALMA\n",
    "    \"haoranxu/ALMA-7B\",\n",
    "    # Llama 3 Instruct\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # Falcon 3 Mamba Instruct\n",
    "    \"tiiuae/Falcon3-Mamba-7B-Instruct\",\n",
    "    # Falcon 3 Instruct\n",
    "    \"tiiuae/Falcon3-1B-Instruct\",\n",
    "    \"tiiuae/Falcon3-3B-Instruct\",\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "    # Qwen 2.5 Mamba Instruct\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    # Mistral Instruct\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    # BayLing\n",
    "    \"ICTNLP/bayling-2-7b\",\n",
    "    # Bloom & Bloomz\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloom-3b\",\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    "    \"bigscience/bloomz-3b\",\n",
    "    \"bigscience/bloomz-7b1\",\n",
    "    # OPT\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"facebook/opt-iml-1.3b\",\n",
    "    # MPT\n",
    "    \"mosaicml/mpt-7b-instruct\",\n",
    "]\n",
    "\n",
    "\n",
    "ds_available = [\"haoranxu/WMT23-Test\",\n",
    "                \"openlanguagedata/flores_plus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "get_input_targets_NLLB,\n",
    "translate_list_of_str_NLLB,\n",
    "translate_batched_NLLB,\n",
    "get_input_targets_ALMA,\n",
    "translate_list_of_str_ALMA,\n",
    "translate_batched_ALMA,\n",
    "get_input_targets_Llama3,\n",
    "extract_translation_Llama3,\n",
    "translate_list_of_str_Llama3,\n",
    "translate_batched_Llama3,\n",
    "get_input_targets_Falcon3,\n",
    "extract_translation_Falcon3Mamba,\n",
    "translate_list_of_str_Falcon3Mamba,\n",
    "translate_batched_Falcon3Mamba,\n",
    "extract_translation_Falcon3,\n",
    "translate_list_of_str_Falcon3,\n",
    "translate_batched_Falcon3,\n",
    "get_input_targets_Qwen2_5,\n",
    "extract_translation_Qwen2_5,\n",
    "translate_list_of_str_Qwen2_5,\n",
    "translate_batched_Qwen2_5,\n",
    "get_input_targets_Mistral,\n",
    "extract_translation_Mistral,\n",
    "translate_list_of_str_Mistral,\n",
    "translate_batched_Mistral,\n",
    "get_input_targets_BayLing,\n",
    "translate_list_of_str_BayLing,\n",
    "translate_batched_BayLing,\n",
    "get_input_targets_BLOOM,\n",
    "translate_list_of_str_BLOOM,\n",
    "translate_batched_BLOOM,\n",
    "get_input_targets_OPT,\n",
    "translate_list_of_str_OPT,\n",
    "translate_batched_OPT,\n",
    "get_input_targets_MPT,\n",
    "translate_list_of_str_MPT,\n",
    "translate_batched_MPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling\n",
    "We use WNT23 from the authors preprocessed split and the FLORES+ dataset, format in the same way that the WNT23 is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reduce_flores_to_some_languages, transform_to_WNT_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNT23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wnt = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "print(len(ds_wnt), ds_wnt[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "ds_flores_reduced = reduce_flores_to_some_languages(ds_flores, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "ds_flores_wnt_style = transform_to_WNT_style(ds_flores, lang=\"zh\", lang_start=\"en\")\n",
    "print(f\"Time to compute: {time.time()-t1:.2f}s\", ds_flores_wnt_style[0:4])\n",
    "\n",
    "t1 = time.time()\n",
    "ds_flores_wnt_style_reduced = transform_to_WNT_style(ds_flores_reduced, lang=\"zh\", lang_start=\"en\")\n",
    "print(f\"Time to compute: {time.time()-t1:.2f}s\", ds_flores_wnt_style_reduced[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_translations_filename\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\"]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None]\n",
    "\n",
    "\n",
    "for dataset_name, reduce_size in zip(dataset_names, reduce_sizes):\n",
    "    for model_name, model_size in zip(model_names, model_sizes):\n",
    "        for direction in directions:\n",
    "            translations_filename = get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size, None)\n",
    "            with open(translations_filename, \"rb\") as f:\n",
    "                tslt = pickle.load(f)\n",
    "            model_size_p = \"-\"+model_size if model_size is not None else \"\"\n",
    "            for i in range(5):\n",
    "                print(f\"[{dataset_name} red_{reduce_size}] [{model_name}{model_size_p}] [{direction} idx {5*i}]\", tslt[5*i])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & inference loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_NLLB(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(translate_list_of_str_NLLB(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and LoRA weights\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_ALMA(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_ALMA(inputs[i:i+1], tokenizer, model, \"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3 Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Llama3(ds_wnt, source_lang=\"en\", target_lang=\"cs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Llama3(inputs[0:5], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3 not instructed 4B (for comparaison to finetuned version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "Q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "                                bnb_4bit_use_double_quant=False)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_targets_Llama3NI4bit(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name} and end your answer as soon as the task is finished:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_Llama3NI4bit(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(**inputs.to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[-1] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_Llama3NI4bit(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Llama3NI4bit(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Llama3NI4bit(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_Llama3NI4bit(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon 3 Instruct (mamba and transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-1B-Instruct\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Falcon3(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Falcon3Mamba(inputs[0:4], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_list_of_str_Falcon3(inputs[0:8], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Qwen2_5(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(translate_list_of_str_Qwen2_5(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import hf_token\n",
    "huggingface_hub.login(token = hf_token)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_Mistral(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_Mistral(inputs[i:i+1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"ICTNLP/bayling-2-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"ICTNLP/bayling-2-7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_BayLing(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(translate_list_of_str_BayLing(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom & Bloom Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b7\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1\")\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_BLOOM(ds_wnt, \"en\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_batched_BLOOM(inputs[0:2], model, tokenizer, batch_size=1, target_language=\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPT & OPT Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", device_map=device)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "# Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-iml-1.3b\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-1.3b\", torch_dtype=\"auto\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_OPT(ds_wnt, \"en\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_batched_OPT(inputs[4:6], model, tokenizer, batch_size=1, target_language=\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, inputs, targets = get_input_targets_MPT(ds_wnt, source_lang=\"en\", target_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,6):\n",
    "    print(translate_list_of_str_MPT(inputs[i:i+1], tokenizer, model, target_language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global benchmark\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\"]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"flores\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For RAG and ICL comparison, 0 examples\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wanted to do but computation time too expansive\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "model_names = [\"bloom\", \"bloom\", \"bloom\", \"bloom\",\n",
    "               \"bloomz\", \"bloomz\",\n",
    "               \"opt\", \"opt\", \"opt\",\n",
    "               \"bayling\",\n",
    "               \"mpt\"]\n",
    "model_sizes = [\"0.5B\", \"1B\", \"3B\", \"7B\",\n",
    "               \"1B\", \"3B\",\n",
    "               \"0.1B\", \"0.3B\", \"7B\",\n",
    "               None,\n",
    "               None]\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"wnt23\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=100)\n",
    "\n",
    "generate_translation_several_models(directions,\n",
    "                                    dataset_name=\"flores\",\n",
    "                                    model_names=model_names,\n",
    "                                    model_sizes=model_sizes,\n",
    "                                    batch_size=1,\n",
    "                                    reduce_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from predictions: evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"bleurt\"]\n",
    "\n",
    "dataset_names = [\"wnt23\"]\n",
    "reduce_sizes = [50]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"rouge\", \"bleu\", \"sacrebleu\", \"chrf\", \"comet\", \"meteor\", \"bertscore\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comput BLEURT metric alone because it is not offloaded of the GPU, need to restart the kernel...\n",
    "metric_names = [\"bleurt\"]\n",
    "\n",
    "dataset_names = [\"flores\", \"wnt23\"]\n",
    "reduce_sizes = [200, 100]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"bloomz\",\n",
    "               \"opt-instruct\"]\n",
    "model_sizes = [\"7B\",\n",
    "               None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel plots with all evaluation trajectories\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"wnt23\", \"flores\"]\n",
    "reduce_sizes = [100, 200]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"dataset\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/all_dataset\")\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/all_direction\")\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"model\"],\n",
    "                    colors=None,\n",
    "                    savepath = \"./results/evaluations_figures/all_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel plot with chinese trajectories\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\"]\n",
    "reduce_sizes = [200]\n",
    "\n",
    "directions = [\"en-zh\", \"zh-en\"]\n",
    "\n",
    "model_names = [\"alma\", \"nllb\",\n",
    "               \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\"]\n",
    "model_sizes = [None, None,\n",
    "               \"8B\",\n",
    "            None,\n",
    "            \"7B\",\n",
    "               \"3B\", \"7B\", \"1.5B\",\n",
    "               None,\n",
    "               \"7B\"]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\", \"model\"],\n",
    "                    colors = plt.cm.tab20.colors,\n",
    "                    savepath = \"./results/evaluations_figures/chinese_dir_model_flores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel plot NLLB v.s. ALMA\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "dataset_names = [\"flores\", \"wnt23\"]\n",
    "reduce_sizes = [200, 100]\n",
    "\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\"]\n",
    "\n",
    "model_sizes = [None,\n",
    "               None]\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_names, reduce_sizes,\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\", \"model\"],\n",
    "                    colors = plt.cm.tab20.colors[::2]+plt.cm.tab20.colors[1::2],\n",
    "                    savepath = \"./results/evaluations_figures/alma-nllb_direction_model\")\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    [\"flores\"], [200],\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\", \"model\"],\n",
    "                    colors = plt.cm.tab20.colors[::2]+plt.cm.tab20.colors[1::2],\n",
    "                    savepath = \"./results/evaluations_figures/alma-nllb_flores_direction_model\")\n",
    "\n",
    "make_parallel_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    [\"wnt23\"], [100],\n",
    "                    metric_names,\n",
    "                    list_colors_per = [\"direction\", \"model\"],\n",
    "                    colors = plt.cm.tab20.colors[::2]+plt.cm.tab20.colors[1::2],\n",
    "                    savepath = \"./results/evaluations_figures/alma-nllb_wnt23_direction_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar plot per metric\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]\n",
    "\n",
    "dataset_name = \"flores\"\n",
    "reduce_size = 200\n",
    "\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "make_bar_plot(directions,\n",
    "                model_names, model_sizes,\n",
    "                dataset_name, reduce_size,\n",
    "                metric_names,\n",
    "                savepath = \"./results/evaluations_figures/barplot_all_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar plot per direction\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "\n",
    "model_names = [\"alma\",\n",
    "               \"nllb\",\n",
    "               \"llama3\", \"llama3\", \"llama3\",\n",
    "               \"falcon3-mamba\",\n",
    "               \"falcon3\", \"falcon3\", \"falcon3\",\n",
    "               \"qwen2.5\", \"qwen2.5\", \"qwen2.5\", \"qwen2.5\",\n",
    "               \"mistral\",\n",
    "               \"bloomz\",\n",
    "               \"opt-instruct\",]\n",
    "model_sizes = [None,\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"8B\",\n",
    "               None,\n",
    "               \"1B\", \"3B\", \"7B\",\n",
    "               \"0.5B\", \"1.5B\", \"3B\", \"7B\",\n",
    "               None,\n",
    "               \"7B\",\n",
    "               None,]\n",
    "\n",
    "dataset_name = \"flores\"\n",
    "reduce_size = 200\n",
    "\n",
    "metric_names = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\",\n",
    "                \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\",\n",
    "                \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "\n",
    "make_bar_plot_all_metrics(directions, model_names, model_sizes, dataset_name, reduce_size, metric_names,\n",
    "                        savepath = \"./results/evaluations_figures/barplot_all_metrics_all_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarck table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "eval_directory = \"./evaluations/\"\n",
    "models = [\"alma\",\"bloomz\",\"falcon3\",\"falcon3\",\"falcon3\",\"falcon3-mamba\",\"llama3\",\"llama3\",\"llama3\",\"mistral\",\"nllb\",\"opt-instruct\",\"qwen2.5\",\"qwen2.5\",\"qwen2.5\",\"qwen2.5\"]\n",
    "models_sizes = [None,\"7B\",\"1B\",\"3B\",\"7B\",None,\"1B\",\"3B\",\"8B\",None,None,None,\"0.5B\",\"1.5B\",\"3B\",\"7B\"]\n",
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics_per_model_direction_dataset(model_name, model_size, direction, dataset_name):\n",
    "    if dataset_name == \"flores\":\n",
    "        red_size = \"200\"\n",
    "    elif dataset_name == \"wnt23\":\n",
    "        red_size = \"100\"\n",
    "    if model_size is not None:\n",
    "        model_name+= f\"-{model_size}\"\n",
    "    eval_file_name = eval_directory + f\"raw_{dataset_name}_{model_name}_{direction}_red-{red_size}.pkl\"\n",
    "    \n",
    "    with open(eval_file_name, \"rb\") as f:\n",
    "        evaluation_pred = pickle.load(f)\n",
    "    \n",
    "    return evaluation_pred\n",
    "\n",
    "def extract_metrics_per_direction_dataset(direction, dataset_name):\n",
    "    df_eval = pd.DataFrame()\n",
    "    for model_name, model_size in zip(models, models_sizes):\n",
    "        eval_dict = extract_metrics_per_model_direction_dataset(model_name,model_size,direction,dataset_name)\n",
    "        eval_dict = dict(map(lambda kv: (kv[0], [kv[1][\"mean_score\"]]), eval_dict.items()))\n",
    "        model_name_ = model_name if model_size is None else f\"{model_name}-{model_size}\"\n",
    "        df_eval = pd.concat([df_eval, pd.DataFrame(eval_dict,index=[model_name_])])\n",
    "\n",
    "    return df_eval\n",
    "\n",
    "def load_evaluations_as_tables(output_directory=\"./results/evaluations_tables/\"):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    for dataset_name in ['wnt23','flores']:\n",
    "        df_eval_list =[]\n",
    "        for direction in directions:\n",
    "            evaluation_file = output_directory + f\"eval_{dataset_name}_{direction}_all-models.csv\"\n",
    "            df_eval = extract_metrics_per_direction_dataset(direction, dataset_name)\n",
    "            df_eval_list.append(df_eval)\n",
    "            df_eval.to_csv(evaluation_file)\n",
    "        df_eval_avg = pd.concat(df_eval_list).groupby(level=0).mean()\n",
    "        df_eval_avg.to_csv(output_directory+f\"eval_{dataset_name}_all-directions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_evaluations_as_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
