{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "results = precision_metric.compute(references=[0, 1], predictions=[0, 1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad\")\n",
    "predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
    "references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "task_evaluator = evaluator(\"automatic-speech-recognition\")\n",
    "data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"validation[:40]\")\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"https://huggingface.co/openai/whisper-tiny.en\",\n",
    "    data=data,\n",
    "    input_column=\"path\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.TranslationEvaluator(model_or_pipeline: typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None,\n",
    "                            data: typing.Union[str, datasets.arrow_dataset.Dataset] = None,\n",
    "                            subset: typing.Optional[str] = Nonesplit: typing.Optional[str] = None,\n",
    "                            metric: typing.Union[str, evaluate.module.EvaluationModule] = None,\n",
    "                            tokenizer: typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None,\n",
    "                            strategy: typing.Literal['simple', 'bootstrap'] = 'simple',\n",
    "                            confidence_level: float = 0.95,\n",
    "                            n_resamples: int = 9999,\n",
    "                            device: int = None,\n",
    "                            random_state: typing.Optional[int] = None,\n",
    "                            input_column: str = 'text',\n",
    "                            label_column: str = 'label',\n",
    "                            generation_kwargs: dict = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tested = [\"en\", \"de\", \"cs\", \"is\", \"zh\", \"ru\"]\n",
    "evaluated_metrics = [\"accuracy\", \"recall\", \"precision\", \"bleu\", \"rouge\",  \"bert_score\", \"meteor\", \"perplexity\", \"comet\", \"bleurt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT dataset loading\n",
    "ds = load_dataset(\"haoranxu/WMT23-Test\", \"{language1}-{language1}\")\n",
    "\n",
    "# FLORES 200 dataset laoding\n",
    "ds = load_dataset(\"haoranxu/FLORES-200\", \"{language1}-{language1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to adapt depending on dataset and article\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True) # -> Voir si ne  peux pas Ãªtre inclus dans la pipeline\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_evaluator = evaluator(\"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluate.TranslationEvaluator(model_or_pipeline=...,\n",
    "                                          data = ds,\n",
    "                                          metric = evaluate.combine(evaluated_metrics),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
