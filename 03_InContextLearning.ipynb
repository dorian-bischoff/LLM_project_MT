{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset, Dataset\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from sacrebleu.tokenizers.tokenizer_13a import Tokenizer13a\n",
    "from sacrebleu.tokenizers.tokenizer_zh import TokenizerZh\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 5\n",
    "max_new_tokens = 512\n",
    "top_p = 0.9\n",
    "temperature = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils from evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################   NLLB\n",
    "\n",
    "def get_input_targets_NLLB(dataset_wnt_format, source_lang, target_lang):\n",
    "    inputs = [example[source_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset_wnt_format[f\"{source_lang}-{target_lang}\"]]\n",
    "    return inputs, inputs, targets\n",
    "\n",
    "def translate_list_of_str_NLLB(list_str, tokenizer, model, to_laguage):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    equivalence_language_to_FLORES = {\"en\": \"eng_Latn\", \"de\": \"deu_Latn\", \"ru\": \"rus_Cyrl\", \"is\": \"isl_Latn\", \"zh\": \"zho_Hans\", \"cs\": \"ces_Latn\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        language_tgt_FLORES = equivalence_language_to_FLORES[to_laguage]\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(language_tgt_FLORES),\n",
    "                                    num_beams=num_beams, max_length=max_new_tokens, early_stopping=True,\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_NLLB(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size = 4\n",
    "    For 16GB VRAM, use batch_size = 8 (better working with unbatch version to avoid pad noise).\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_NLLB(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   ALMA\n",
    "\n",
    "def get_input_targets_ALMA(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_ALMA(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_ALMA(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM, use batch_size=1\n",
    "    For 16GB VRAM, use batch_size=3\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_ALMA(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Llama 3\n",
    "\n",
    "def get_input_targets_Llama3(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "        \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Llama3(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|end_of_text|>\")[0]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>assistant\\n\")[-1]\n",
    "    translation_only = translation_only.split(\"<|eot_id|><|start_header_id|>\")[-1]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Llama3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\")\n",
    "        out_tokens = model.generate(**tokens.to(device),\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Llama3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Llama3(inputs, model, tokenizer, batch_size, target_language):\n",
    "    \"\"\"\n",
    "    For 8GB VRAM use\n",
    "        batch_size=20 with Llama3 1B,\n",
    "        batch_size=4 with Llama3 3B\n",
    "    For 16 GB VRAM use \n",
    "        batch_size=40 with Llama3 1B,\n",
    "        batch_size=10 with Llama3 3B,\n",
    "        batch_size=5 with Llama3 8B,\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Llama3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language=None)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Falcon 3 (Normal + Mamba)\n",
    "\n",
    "def get_input_targets_Falcon3(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Falcon3Mamba(translated_prompt):\n",
    "    answer = translated_prompt.split(\"<|im_end|>\\n<|im_start|>assistant\\n\")[-1]\n",
    "    translation_only = answer.split(\"<|im_end|>\")[0]\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Falcon3Mamba(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3Mamba(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3Mamba(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=4 with Falcon Mamba 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon Mamba 7B (4 bits quantization),\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3Mamba(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "def extract_translation_Falcon3(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|assistant|>\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|pad|>\")[-1]\n",
    "    translation_only = answer.split(\"<|endoftext|>\")[0]\n",
    "    translation_only = re.sub(r\"^[^a-zA-Z0-9]*\", \"\", translation_only)\n",
    "    return translation_only.replace(\"assistant|>\\n\", \"\")\n",
    "\n",
    "def translate_list_of_str_Falcon3(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Falcon3(trans) for trans in translations]\n",
    "        return translations\n",
    "    \n",
    "def translate_batched_Falcon3(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use\n",
    "        batch_size=8 with Falcon 7B (8 bits quantization),\n",
    "        batch_size=4 with Falcon 3B,\n",
    "        batch_size=12 with Falcon 1B\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Falcon3(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Qwen 2.5\n",
    "\n",
    "def get_input_targets_Qwen2_5(dataset, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    This function is valid for Falcon 3 and it mamba version\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Qwen2_5(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"\\n<|im_start|>assistant\\n\")[-1]\n",
    "    answer = answerpadded.split(\"<|im_end|>\")[0]\n",
    "    translation_only = answer.replace(\"<|endoftext|>\", \"\")\n",
    "    return translation_only\n",
    "\n",
    "def translate_list_of_str_Qwen2_5(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Qwen2_5(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Qwen2_5(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=100 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Qwen2_5(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   Mistral\n",
    "\n",
    "\n",
    "def get_input_targets_Mistral(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [\n",
    "        [{\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "         {\"role\": \"user\",\n",
    "          \"content\": f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "          + f\"{example.get(source_lang)}\"\n",
    "        }] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def extract_translation_Mistral(translated_prompt):\n",
    "    answerpadded = translated_prompt.split(\"[/INST] \")[-1]\n",
    "    answer = answerpadded.split(\"</s>\")[0]\n",
    "    return answer\n",
    "\n",
    "def translate_list_of_str_Mistral(list_str, tokenizer, model, target_language=None):\n",
    "    with torch.no_grad():\n",
    "        instruct_messages = tokenizer.apply_chat_template(list_str, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer(instruct_messages, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
    "        out_tokens = model.generate(**tokens,\n",
    "                                    num_beams=num_beams, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        translations = tokenizer.batch_decode(out_tokens)\n",
    "        translations = [extract_translation_Mistral(trans) for trans in translations]\n",
    "        return translations\n",
    "\n",
    "def translate_batched_Mistral(inputs, model, tokenizer, batch_size, target_language=None):\n",
    "    \"\"\"\n",
    "    For 16GB VRAM, use batch_size=2 (up to)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_Mistral(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   BayLing\n",
    "\n",
    "def get_input_targets_BayLing(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_BayLing(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_BayLing(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_BayLing(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   BLOOM & BLOOMZ\n",
    "\n",
    "def get_input_targets_BLOOM(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_BLOOM(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_BLOOM(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_BLOOM(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   OPT & OPT Instruct\n",
    "\n",
    "def get_input_targets_OPT(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        f\"Translate from {source_lang_name} to {target_lang_name}:\"\n",
    "        + f\"\\n{source_lang_name}: {example.get(source_lang)} \\n{target_lang_name}:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_OPT(list_str, tokenizer, model, target_language):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        tgt_language_name = language_name[target_language]\n",
    "        translated_text = [t.split(f\"{tgt_language_name}:\")[2] for t in translated_text] # Remove prompt\n",
    "        translated_text = [t.split(f\"\\n[END]\")[0] for t in translated_text]\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_OPT(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_OPT(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds\n",
    "\n",
    "#################################   MPT\n",
    "\n",
    "def get_input_targets_MPT(dataset, source_lang, target_lang):\n",
    "    language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "    source_lang_name = language_name[source_lang]\n",
    "    target_lang_name = language_name[target_lang]\n",
    "    # Use the instruct template\n",
    "    sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    inputs = [(\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n### Instruction:\"\n",
    "        + f\"Translate from {source_lang_name} to {target_lang_name}: {example.get(source_lang)}\"\n",
    "        + \"\\n### Response:\")\n",
    "        for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "    return sources, inputs, targets\n",
    "\n",
    "def translate_list_of_str_MPT(list_str, tokenizer, model, target_language=None):\n",
    "    \"\"\"\n",
    "    Returns a list containing str corresponding to translation of the inputted\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(list_str, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(inputs[\"input_ids\"].to(device),\n",
    "                                    num_beams=num_beams, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "                                    temperature=temperature, top_p=top_p\n",
    "                                    ).cpu()\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translated_text = [t.split(\"\\n### Response:\")[-1] for t in translated_text] # Remove prompt\n",
    "    return translated_text\n",
    "\n",
    "def translate_batched_MPT(inputs, model, tokenizer, batch_size, target_language):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(inputs)//batch_size)):\n",
    "        tslt = translate_list_of_str_MPT(inputs[i*batch_size : (i+1)*batch_size], tokenizer, model, target_language)\n",
    "        preds.extend(tslt)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_flores_to_some_languages(ds_flores, directions: list[str]):\n",
    "    \"\"\"\n",
    "    Extracts a subpart of FLORES dataset to group computations. Keep only the languages\n",
    "    presents in directions\n",
    "    Returns a dataset\n",
    "    \"\"\"\n",
    "    print(\"Extracting all languages in directions from FLORES...\")\n",
    "    list_languages = []\n",
    "    for direction in directions:\n",
    "        lang1, lang2 = direction[0:2], direction[3:5]\n",
    "        if lang1 not in list_languages:\n",
    "            list_languages.append(lang1)\n",
    "        if lang2 not in list_languages:\n",
    "            list_languages.append(lang2)\n",
    "\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    ds_list = []\n",
    "    for elem in ds_flores:\n",
    "        for lang in list_languages:\n",
    "            if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "                if lang == \"zh\":\n",
    "                    if elem[\"glottocode\"] == \"beij1234\":\n",
    "                        ds_list.append(elem)\n",
    "                else:\n",
    "                    ds_list.append(elem)\n",
    "    return Dataset.from_list(ds_list)\n",
    "\n",
    "def transform_to_WNT_style(ds_flores, lang, lang_start=\"en\"):\n",
    "    \"\"\"\n",
    "    Convert FLORES dataset (or a fraction of it) to a dataset formatted as WNT23\n",
    "    Returns a dataset\n",
    "    \"\"\"\n",
    "    language_to_iso = {\"en\": \"eng\", \"de\": \"deu\", \"cs\": \"ces\", \"is\": \"isl\", \"zh\": \"cmn\", \"ru\": \"rus\"}\n",
    "    list_sentence_lang, list_sentence_lang_start = [], []\n",
    "    for elem in ds_flores:\n",
    "        if elem[\"iso_639_3\"] == language_to_iso[lang]:\n",
    "            if lang == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang.append(elem[\"text\"])\n",
    "\n",
    "        elif elem[\"iso_639_3\"] == language_to_iso[lang_start]:\n",
    "            if lang_start == \"zh\":\n",
    "                if elem[\"glottocode\"] == \"beij1234\":\n",
    "                    list_sentence_lang_start.append(elem[\"text\"])\n",
    "            else:\n",
    "                list_sentence_lang_start.append(elem[\"text\"])\n",
    "    assert len(list_sentence_lang) == len(list_sentence_lang_start)\n",
    "    #print(f\"Number of samples: {len(list_sentence_lang)}\")\n",
    "    final_text_list = []\n",
    "    for i in range(len(list_sentence_lang)):\n",
    "        final_text_list.append({f\"{lang_start}\": list_sentence_lang_start[i],\n",
    "                                f\"{lang}\": list_sentence_lang[i],})\n",
    "    return Dataset.from_dict({f\"{lang_start}-{lang}\": final_text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_benchmark(model_name: str, model_size: Union[str, None] = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Load model and tokenizer for the models considered in the benchmark\n",
    "    Returns (tokenizer, model)\n",
    "    \"\"\"\n",
    "    if model_name == \"alma\":\n",
    "        tokenizer = transformers.LlamaTokenizer.from_pretrained(\"haoranxu/ALMA-7B\", padding_side='left')\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True) \n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        \n",
    "    elif model_name == \"nllb\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", device_map=device)\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"meta-llama/Llama-3.2-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "            nQ_cofig = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    elif model_name == \"llama3-NI-4bit\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "        nQ_cofig = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                      bnb_4bit_quant_type=\"nf4\",\n",
    "                                      bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "                                      bnb_4bit_use_double_quant=False)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    elif model_name == \"falcon3-mamba\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\")\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-Mamba-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"falcon3\":\n",
    "        if model_size==\"1B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"tiiuae/Falcon3-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"tiiuae/Falcon3-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    elif model_name == \"qwen2.5\":\n",
    "        if model_size==\"0.5B\" or model_size==\"1.5B\" or model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(f\"Qwen/Qwen2.5-{model_size}-Instruct\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"mistral\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    elif model_name == \"bayling\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"ICTNLP/bayling-2-7b\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"ICTNLP/bayling-2-7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"bloom\":\n",
    "        if model_size==\"0.5B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "        elif model_size==\"1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "\n",
    "    elif model_name == \"bloomz\":\n",
    "        if model_size==\"1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b7\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-1b7\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-7b1\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"opt\":\n",
    "        if model_size==\"0.1B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", device_map=device)\n",
    "        elif model_size==\"0.3B\":\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", device_map=device)\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "            Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "    \n",
    "    elif model_name == \"opt-instruct\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/opt-iml-1.3b\")\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-1.3b\", torch_dtype=\"auto\", device_map=device)\n",
    "    \n",
    "    elif model_name == \"mpt\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-instruct\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        Q_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\", torch_dtype=\"auto\", device_map=device, quantization_config=Q_config)\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    return tokenizer, model\n",
    "\n",
    "def get_support_fn_benchmark(model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Return functions to generate rightly formatted inputs and a function to use\n",
    "    translation models in inference for the models considered in the benchmark\n",
    "    \"\"\"\n",
    "    if model_name == \"alma\":\n",
    "        get_input_targets_fn = get_input_targets_ALMA\n",
    "        tslt_fn = translate_batched_ALMA\n",
    "        \n",
    "    elif model_name == \"nllb\":\n",
    "        get_input_targets_fn = get_input_targets_NLLB\n",
    "        tslt_fn = translate_batched_NLLB\n",
    "\n",
    "    elif model_name == \"llama3\":\n",
    "        get_input_targets_fn = get_input_targets_Llama3\n",
    "        tslt_fn = translate_batched_Llama3\n",
    "    \n",
    "    elif model_name == \"llama3-NI-4bit\":\n",
    "        get_input_targets_fn = get_input_targets_Llama3NI4bit\n",
    "        tslt_fn = translate_batched_Llama3NI4bit\n",
    "    \n",
    "    elif model_name == \"falcon3-mamba\":\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3Mamba\n",
    "    \n",
    "    elif model_name == \"falcon3\":\n",
    "        get_input_targets_fn = get_input_targets_Falcon3\n",
    "        tslt_fn = translate_batched_Falcon3\n",
    "    \n",
    "    elif model_name == \"qwen2.5\":\n",
    "        get_input_targets_fn = get_input_targets_Qwen2_5\n",
    "        tslt_fn = translate_batched_Qwen2_5\n",
    "    \n",
    "    elif model_name == \"mistral\":\n",
    "        get_input_targets_fn = get_input_targets_Mistral\n",
    "        tslt_fn = translate_batched_Mistral\n",
    "    \n",
    "    elif model_name == \"bayling\":\n",
    "        get_input_targets_fn = get_input_targets_BayLing\n",
    "        tslt_fn = translate_batched_BayLing\n",
    "\n",
    "    elif model_name == \"bloom\" or model_name == \"bloomz\":\n",
    "        get_input_targets_fn = get_input_targets_BLOOM\n",
    "        tslt_fn = translate_batched_BLOOM\n",
    "    \n",
    "    elif model_name == \"opt\" or model_name == \"opt-instruct\":\n",
    "        get_input_targets_fn = get_input_targets_OPT\n",
    "        tslt_fn = translate_batched_OPT\n",
    "    \n",
    "    elif model_name == \"mpt\":\n",
    "        get_input_targets_fn = get_input_targets_MPT\n",
    "        tslt_fn = translate_batched_MPT\n",
    "        \n",
    "    return get_input_targets_fn, tslt_fn\n",
    "\n",
    "def get_inp_tgt_lang(direction: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return source and target language given a direction xx-yy\n",
    "    \"\"\"\n",
    "    return direction[0:2], direction[3:5]\n",
    "\n",
    "def reduce_dataset(inputs: list[str], sources: list[str], targets, final_nb: list[str]) -> tuple[list[str], list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Selects randomly the samples of the evaluation corpus\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(inputs))\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(idx, final_nb)\n",
    "    return [inputs[i] for i in idx], [sources[i] for i in idx], [targets[i] for i in idx]\n",
    "\n",
    "def get_translations_filename(direction: str, dataset_name: str, model_name: str, model_size: Union[str, None], reduce_size: Union[int, None], translation_folder: Union[str, None] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate the pkl filename where to save the generated translations\n",
    "    \"\"\"\n",
    "    mod_size = \"-\"+model_size if model_size is not None else \"\"\n",
    "    translation_folder = \"evaluations\" if translation_folder is None else translation_folder\n",
    "    return f\"./generated_translations/{translation_folder}/{dataset_name}_{model_name}{mod_size}_{direction}_red-{reduce_size}.pkl\"\n",
    "\n",
    "def get_eval_filename(direction: str, dataset_name: str, model_name: str, model_size: Union[str, None], reduce_size: Union[int, None]) -> str:\n",
    "    \"\"\"\n",
    "    Generate the pkl filename where to save the computed metrics\n",
    "    \"\"\"\n",
    "    mod_size = \"-\"+model_size if model_size is not None else \"\"\n",
    "    return f\"./evaluations/raw_{dataset_name}_{model_name}{mod_size}_{direction}_red-{reduce_size}.pkl\"\n",
    "\n",
    "\n",
    "# Main function to generate translations\n",
    "def generate_translation_different_directions(directions: list[str],\n",
    "                                              dataset_name: str,\n",
    "                                              model_name: str,\n",
    "                                              batch_size: int,\n",
    "                                              reduce_size: Union[int, None] = None,\n",
    "                                              model_size: Union[str, None] = None,\n",
    "                                              load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                              get_input_targets_fn = None,\n",
    "                                              tslt_fn = None,\n",
    "                                              translation_folder = None) -> None:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - directions: list of strings\n",
    "        - dataset name: str, either \"flores\" or \"wnt23\"\n",
    "        - model_name: str\n",
    "        - batch_size: int (advised 1 to avoid padding - or make sure your tokenizer is correctly parametrized)\n",
    "        - reduce_size: int, the number of random samples to use. Samples are sampled using seed to have same\n",
    "          samples for each models. If reduce_size=None, take all the dataset samples.\n",
    "        - model_size: str or None\n",
    "\n",
    "        - load_model_and_tokenizer_fn: a function returning a tuple\n",
    "          SIGNATURE : load_model_and_tokenizer_fn(model_name: str, model_size: Union[str, None]) -> tokenizer, model\n",
    "\n",
    "        - get_input_targets_fn: a function returning a tuple of three lists of str gicen the dataset and the source and target language:\n",
    "          SIGNATURE : get_input_targets_fn(ds: HF_dataset, input_language: str, target_language: str) -> sources, inputs, targets: list[str], list[str], list[str]\n",
    "            sources are the initial sentences (used in COMET metric)\n",
    "            inputs are the complete prompts to the model (only one string, apply the instruct template in get_input_targets_fn)\n",
    "            targets are the target translations\n",
    "\n",
    "        - tslt_fn: a function prompting the model and generating the a list of translations given a list of prompt, the tokenizer and the model.\n",
    "          It must include a batch_size argument (the batched processing is not necessary to implement in the function). Include also the\n",
    "          target_language as argument for consistency with other functions.\n",
    "          SIGNATURE : tslt_fn(inputs: list[str], model: HF_model, tokenizer: HF_tokenizer, batch_size: int, target_language: Union[str, None]) -> translation_pred: list[str]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "\n",
    "    # Loading corresponding model\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer, model = load_model_and_tokenizer_fn(model_name, model_size)\n",
    "    if get_input_targets_fn is None:\n",
    "        get_input_targets_fn, tslt_fn = get_support_fn_benchmark(model_name)\n",
    "\n",
    "    for direction in directions:\n",
    "        print(f\"Translating {direction} with model {model_name}\"\n",
    "              +(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "              +f\" for dataset {dataset_name}...\")\n",
    "        input_language, target_language = get_inp_tgt_lang(direction)\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            if direction != \"cs-en\":\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "            else:\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "                ds = Dataset.from_dict({f\"cs-en\": ds[\"en-cs\"][::-1]}) # Reverse list to avoid having same sentences (if reduce_size not None)\n",
    "        # Extracting input & targets\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        print(f\"Total number of samples: {len(sources)}\" + (\"\" if reduce_size is None else f\"; reduced to {reduce_size} (numpy seed = 42)\"))\n",
    "        if reduce_size is not None:\n",
    "            sources, inputs, targets = reduce_dataset(sources, inputs, targets, reduce_size)\n",
    "        translation_pred = tslt_fn(inputs, model, tokenizer, batch_size, target_language)\n",
    "\n",
    "        # Saving translations\n",
    "        translation_folder = \"evaluations\" if translation_folder is None else translation_folder\n",
    "        if not os.path.exists(f\"./generated_translations/{translation_folder}\"):\n",
    "            os.makedirs(f\"./generated_translations/{translation_folder}\")\n",
    "        translations_filename = get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size, translation_folder)\n",
    "        \n",
    "        with open(translations_filename, \"wb\") as f:\n",
    "            pickle.dump(translation_pred, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # De-load model from GPU to enable calling this function with another model without restarting kernel\n",
    "    model.cpu()\n",
    "    del model, tokenizer\n",
    "\n",
    "# Wrappers for several models and several datasets\n",
    "def generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                        load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                        get_input_targets_fn = None,\n",
    "                                        tslt_fn = None,\n",
    "                                        translation_folder = None) -> None:\n",
    "    for model_name, model_size in zip(model_names, model_sizes):\n",
    "        generate_translation_different_directions(directions=directions,\n",
    "                                                dataset_name=dataset_name,\n",
    "                                                model_name=model_name,\n",
    "                                                model_size=model_size,\n",
    "                                                batch_size=batch_size,\n",
    "                                                reduce_size=reduce_size,\n",
    "                                                load_model_and_tokenizer_fn = load_model_and_tokenizer_fn,\n",
    "                                                get_input_targets_fn = get_input_targets_fn,\n",
    "                                                tslt_fn = tslt_fn,\n",
    "                                                translation_folder = translation_folder)\n",
    "        \n",
    "def generate_translation_several_datasets(directions, dataset_names, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                          load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                          get_input_targets_fn = None,\n",
    "                                          tslt_fn = None,\n",
    "                                          translation_folder = None) -> None:\n",
    "    for dataset_name in dataset_names:\n",
    "        generate_translation_several_models(directions, dataset_name, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                            load_model_and_tokenizer_fn = load_model_and_tokenizer_fn,\n",
    "                                            get_input_targets_fn = get_input_targets_fn,\n",
    "                                            tslt_fn = tslt_fn,\n",
    "                                            translation_folder = translation_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute metrics given:\n",
    "#   the metric,\n",
    "#   the list of initial sentence,\n",
    "#   the list of target translations,\n",
    "#   the list of translated sentences,\n",
    "#   the target language\n",
    "\n",
    "# Each metric is computed sample by sample. The output is a dictionnary containing\n",
    "#   the full list of scores\n",
    "#   the mean score\n",
    "#   the standard deviation\n",
    "#   the unbias standard deviation\n",
    "\n",
    "def eval_rouge(metric, sources, targets, translation_infered, target_language):\n",
    "    out_rouge = metric.compute(predictions=translation_infered,\n",
    "                                  references=targets,\n",
    "                                  use_aggregator=False)\n",
    "    # For further statistical treatment\n",
    "    results_rouge = {\"rouge1\": {},\n",
    "                     \"rouge2\": {},\n",
    "                     \"rougeL\": {},\n",
    "                     \"rougeLsum\": {},}\n",
    "    for key in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "        results_rouge[key][\"mean_score\"] = np.mean(out_rouge[key]).item()\n",
    "        results_rouge[key][\"std_score\"] = np.std(out_rouge[key]).item()\n",
    "        results_rouge[key][\"std_unbias_score\"] = np.std(out_rouge[key], ddof=1).item()\n",
    "    return results_rouge\n",
    "\n",
    "def eval_bleu(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bleu = {\"scores\": [], \"brevity_penalty\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            bleu_out = metric.compute(predictions=[trans],\n",
    "                                    references=[[tgt]],\n",
    "                                    tokenizer = TokenizerZh() if target_language==\"zh\" else Tokenizer13a())\n",
    "        except ZeroDivisionError:\n",
    "            bleu_out={\"bleu\": 0., \"brevity_penalty\": 0.}\n",
    "\n",
    "        results_bleu[\"scores\"].append(bleu_out[\"bleu\"])\n",
    "        results_bleu[\"brevity_penalty\"].append(bleu_out[\"brevity_penalty\"])\n",
    "    # For further statistical treatment\n",
    "    results_bleu[\"mean_score\"] = np.mean(results_bleu[\"scores\"]).item()\n",
    "    results_bleu[\"std_score\"] = np.std(results_bleu[\"scores\"]).item()\n",
    "    results_bleu[\"std_unbias_score\"] = np.std(results_bleu[\"scores\"], ddof=1).item()\n",
    "    return {\"bleu\": results_bleu}\n",
    "\n",
    "def eval_sacrebleu(metric, sources, targets, translation_infered, target_language):\n",
    "    results_sacrebleu = {\"scores\": [], \"brevity_penalty\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            sacrebleu_out = metric.compute(predictions=[trans],\n",
    "                                            references=[[tgt]],\n",
    "                                            tokenize = \"zh\" if target_language==\"zh\" else \"13a\")\n",
    "        except ZeroDivisionError:\n",
    "            sacrebleu_out = {\"score\": 0., \"bp\": 0.}\n",
    "        results_sacrebleu[\"scores\"].append(sacrebleu_out[\"score\"])\n",
    "        results_sacrebleu[\"brevity_penalty\"].append(sacrebleu_out[\"bp\"])\n",
    "    # For further statistical treatment\n",
    "    results_sacrebleu[\"mean_score\"] = np.mean(results_sacrebleu[\"scores\"]).item()\n",
    "    results_sacrebleu[\"std_score\"] = np.std(results_sacrebleu[\"scores\"]).item()\n",
    "    results_sacrebleu[\"std_unbias_score\"] = np.std(results_sacrebleu[\"scores\"], ddof=1).item()\n",
    "    return {\"sacrebleu\": results_sacrebleu}\n",
    "\n",
    "def eval_chrf_and_chrfplusplus(metric, sources, targets, translation_infered, target_language):\n",
    "    results_chrf = {\"scores\": []}\n",
    "    results_chrfplusplus = {\"scores\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        try:\n",
    "            chrf_out = metric.compute(predictions=[trans],\n",
    "                                    references=[[tgt]],\n",
    "                                    word_order=0,\n",
    "                                    eps_smoothing=False)\n",
    "        except ZeroDivisionError:\n",
    "            chrf_out = {\"score\": 0.}\n",
    "        try:\n",
    "            chrfplusplus_out = metric.compute(predictions=[trans],\n",
    "                                            references=[[tgt]],\n",
    "                                            word_order=2,\n",
    "                                            eps_smoothing=True)\n",
    "        except ZeroDivisionError:\n",
    "            chrfplusplus_out = {\"score\": 0.}\n",
    "        results_chrf[\"scores\"].append(chrf_out['score'])\n",
    "        results_chrfplusplus[\"scores\"].append(chrfplusplus_out['score'])\n",
    "    # For further statistical treatment\n",
    "    results_chrf[\"mean_score\"] = np.mean(results_chrf[\"scores\"]).item()\n",
    "    results_chrf[\"std_score\"] = np.std(results_chrf[\"scores\"]).item()\n",
    "    results_chrf[\"std_unbias_score\"] = np.std(results_chrf[\"scores\"], ddof=1).item()\n",
    "    results_chrfplusplus[\"mean_score\"] = np.mean(results_chrfplusplus[\"scores\"]).item()\n",
    "    results_chrfplusplus[\"std_score\"] = np.std(results_chrfplusplus[\"scores\"]).item()\n",
    "    results_chrfplusplus[\"std_unbias_score\"] = np.std(results_chrfplusplus[\"scores\"], ddof=1).item()\n",
    "    return {\"chrf\": results_chrf,\n",
    "            \"chrfplusplus\": results_chrfplusplus}\n",
    "\n",
    "def eval_comet(metric, sources, targets, translation_infered, target_language):\n",
    "    results_comet = metric.compute(predictions=translation_infered,\n",
    "                                         references=targets,\n",
    "                                         sources=sources)\n",
    "    # For further statistical treatment\n",
    "    results_comet.update({\"std_score\": np.std(results_comet[\"scores\"]).item(),\n",
    "                          \"std_unbias_score\": np.std(results_comet[\"scores\"], ddof=1).item()})\n",
    "    return {\"comet\": results_comet}\n",
    "\n",
    "def eval_bleurt(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bleurt = metric.compute(predictions=translation_infered,\n",
    "                                    references=targets)\n",
    "    # For further statistical treatment\n",
    "    results_bleurt.update({\"mean_score\": np.mean(results_bleurt[\"scores\"]).item(),\n",
    "                           \"std_score\": np.std(results_bleurt[\"scores\"]).item(),\n",
    "                           \"std_unbias_score\": np.std(results_bleurt[\"scores\"], ddof=1).item()})\n",
    "    return {\"bleurt\": results_bleurt}\n",
    "\n",
    "def eval_bertscore(metric, sources, targets, translation_infered, target_language):\n",
    "    results_bert = metric.compute(predictions=translation_infered, references=targets, lang=target_language)\n",
    "    # For further statistical treatment\n",
    "    results_bert.update({\"mean_score\": np.mean(results_bert[\"f1\"]).item(),\n",
    "                         \"std_score\": np.std(results_bert[\"f1\"]).item(),\n",
    "                         \"std_unbias_score\": np.std(results_bert[\"f1\"], ddof=1).item()})\n",
    "    return {\"bertscore\": results_bert}\n",
    "\n",
    "def eval_meteor(metric, sources, targets, translation_infered, target_language):\n",
    "    results_meteor = {\"scores\": []}\n",
    "    for trans, tgt in zip(translation_infered, targets):\n",
    "        meteor_out = metric.compute(predictions=[trans],\n",
    "                                    references=[tgt])\n",
    "        results_meteor[\"scores\"].append(meteor_out[\"meteor\"])\n",
    "    # For further statistical treatment\n",
    "    results_meteor[\"mean_score\"] = np.mean(results_meteor[\"scores\"]).item()\n",
    "    results_meteor[\"std_score\"] = np.std(results_meteor[\"scores\"]).item()\n",
    "    results_meteor[\"std_unbias_score\"] = np.std(results_meteor[\"scores\"], ddof=1).item()\n",
    "    return {\"meteor\": results_meteor}\n",
    "\n",
    "def get_eval_fn(metric_name):\n",
    "    if metric_name == \"rouge\":\n",
    "        return eval_rouge\n",
    "    elif metric_name == \"bleu\":\n",
    "        return eval_bleu\n",
    "    elif metric_name == \"sacrebleu\":\n",
    "        return eval_sacrebleu\n",
    "    elif metric_name == \"chrf\":\n",
    "        return eval_chrf_and_chrfplusplus\n",
    "    elif metric_name == \"comet\":\n",
    "        return eval_comet\n",
    "    elif metric_name == \"bleurt\":\n",
    "        return eval_bleurt\n",
    "    elif metric_name == \"bertscore\":\n",
    "        return eval_bertscore\n",
    "    elif metric_name == \"meteor\":\n",
    "        return eval_meteor\n",
    "\n",
    "def load_metric(metric_name):\n",
    "    if metric_name == \"rouge\":\n",
    "        return evaluate.load('rouge')\n",
    "    elif metric_name == \"bleu\":\n",
    "        return evaluate.load(\"bleu\")\n",
    "    elif metric_name == \"sacrebleu\":\n",
    "        return evaluate.load(\"sacrebleu\")\n",
    "    elif metric_name == \"chrf\":\n",
    "        return evaluate.load(\"chrf\")\n",
    "    elif metric_name == \"comet\":\n",
    "        return evaluate.load('comet')\n",
    "    elif metric_name == \"bleurt\":\n",
    "        return evaluate.load('bleurt', 'bleurt-large-512')\n",
    "    elif metric_name == \"bertscore\":\n",
    "        return evaluate.load(\"bertscore\")\n",
    "    elif metric_name == \"meteor\":\n",
    "        return evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_metric_one_model(metric_name: str, metric, directions: list[str], dataset_name: str, model_name: str, model_size: Union[str, None], reduce_size: Union[int, None],\n",
    "                              input_and_generate_fn = get_support_fn_benchmark) -> None:\n",
    "    \"\"\"\n",
    "    Compute the evaluation accoreding to one metric of one model on several directions\n",
    "    Metric name should be in [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\", \"BLEU\", \"SacreBLEU\", \"chrF\", \"chrF++\", \"COMET\", \"BLEURT\", \"BERTscore\", \"METEOR\"]\n",
    "    metric if the huggingface metric return by evaluate.load()\n",
    "    Refer to translations generation function for input_and_generate_fn\n",
    "\n",
    "    Save directly the computed evaluations, returns None\n",
    "    \"\"\"\n",
    "    # Getting right evaluation function\n",
    "    metric_eval_fn = get_eval_fn(metric_name)\n",
    "\n",
    "    # Loading full flores (if necessary)\n",
    "    if dataset_name == \"flores\":\n",
    "        from credentials import hf_token\n",
    "        huggingface_hub.login(token = hf_token)\n",
    "        ds_flores = load_dataset(\"openlanguagedata/flores_plus\")[\"devtest\"]\n",
    "        ds_flores = reduce_flores_to_some_languages(ds_flores, directions)\n",
    "\n",
    "    for direction in directions:\n",
    "        print(f\"Evaluating translations {direction} with model {model_name}\"\n",
    "              +(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "              +f\" for dataset {dataset_name}...\")\n",
    "        input_language, target_language = get_inp_tgt_lang(direction)\n",
    "\n",
    "        # Loading previous eval if existing\n",
    "        eval_filename = get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "        if not os.path.exists(f\"./evaluations\"):\n",
    "            os.makedirs(f\"./evaluations\")\n",
    "        if os.path.exists(eval_filename):\n",
    "            with open(eval_filename, \"rb\") as f:\n",
    "                complete_eval = pickle.load(f)\n",
    "        else:\n",
    "            complete_eval = {}\n",
    "        \n",
    "        # Getting the right split corresponding to the translation direction\n",
    "        if dataset_name == \"flores\":\n",
    "            ds = transform_to_WNT_style(ds_flores, lang=target_language, lang_start=input_language)\n",
    "        elif dataset_name == \"wnt23\":\n",
    "            if direction != \"cs-en\":\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", direction)[\"test\"]\n",
    "            else:\n",
    "                ds = load_dataset(\"haoranxu/WMT23-Test\", \"en-cs\")[\"test\"]\n",
    "                ds = Dataset.from_dict({f\"cs-en\": ds[\"en-cs\"][::-1]}) # Reverse list to avoid having same sentences (if reduce_size not None)\n",
    "        \n",
    "        # Extracting input & targets\n",
    "        get_input_targets_fn, _ = input_and_generate_fn(model_name)\n",
    "        sources, inputs, targets = get_input_targets_fn(ds, input_language, target_language)\n",
    "        print(f\"Total number of samples: {len(sources)}\" + (\"\" if reduce_size is None else f\"; reduced to {reduce_size} (numpy seed = 42)\"))\n",
    "        if reduce_size is not None:\n",
    "            # /!\\ Use same reduce size and same seed to ensure sources and previous inputs are the same /!\\\n",
    "            sources, inputs, targets = reduce_dataset(sources, inputs, targets, reduce_size)\n",
    "\n",
    "        # Loading precomputed translations\n",
    "        translations_filename = get_translations_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "        with open(translations_filename, \"rb\") as f:\n",
    "            translation_pred = pickle.load(f)\n",
    "        \n",
    "        # Evaluation translation for this direction\n",
    "        eval_dict = metric_eval_fn(metric, sources, targets, translation_pred, target_language)\n",
    "        complete_eval.update(eval_dict)\n",
    "\n",
    "        with open(eval_filename, \"wb\") as f:\n",
    "            pickle.dump(complete_eval, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Wrapper to perform several evaluations\n",
    "def eval_one_metric(metric_name, directions, dataset_names, model_names, model_sizes, reduce_sizes):\n",
    "    print(f\"Computing evaluations with {metric_name}...\")\n",
    "    metric = load_metric(metric_name)\n",
    "    for dataset_name, reduce_size in zip(dataset_names, reduce_sizes):\n",
    "        for model_name, model_size in zip(model_names, model_sizes):\n",
    "            eval_one_metric_one_model(metric_name, metric, directions, dataset_name, model_name, model_size, reduce_size)\n",
    "\n",
    "def eval_metrics(metric_names, directions, dataset_names, model_names, model_sizes, reduce_sizes):\n",
    "    for metric_name in metric_names:\n",
    "        eval_one_metric(metric_name, directions, dataset_names, model_names, model_sizes, reduce_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelCoordinatesPlot(title, N, data, category, ynames, colors=None, category_names=None, savepath=None):\n",
    "    \"\"\"\n",
    "    A legend is added, if category_names is not None.\n",
    "\n",
    "    :param title: The title of the plot.\n",
    "    :param N: Number of data sets (i.e., lines).\n",
    "    :param data: A list containing one array per parallel axis, each containing N data points.\n",
    "    :param category: An array containing the category of each data set.\n",
    "    :param category_names: Labels of the categories. Must have the same length as set(category).\n",
    "    :param ynames: The labels of the parallel axes.\n",
    "    :param colors: A colormap to use.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, host = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "    # organize the data\n",
    "    ys = np.dstack(data)[0]\n",
    "    ymins = ys.min(axis=0)\n",
    "    ymaxs = ys.max(axis=0)\n",
    "    dys = ymaxs - ymins\n",
    "    ymins -= dys * 0.05  # add 5% padding below and above\n",
    "    ymaxs += dys * 0.05\n",
    "    dys = ymaxs - ymins\n",
    "\n",
    "    # transform all data to be compatible with the main axis\n",
    "    zs = np.zeros_like(ys)\n",
    "    zs[:, 0] = ys[:, 0]\n",
    "    zs[:, 1:] = (ys[:, 1:] - ymins[1:]) / dys[1:] * dys[0] + ymins[0]\n",
    "\n",
    "    axes = [host] + [host.twinx() for i in range(ys.shape[1] - 1)]\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_ylim(ymins[i], ymaxs[i])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        if ax != host:\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "            ax.spines[\"right\"].set_position((\"axes\", i / (ys.shape[1] - 1)))\n",
    "\n",
    "    host.set_xlim(0, ys.shape[1] - 1)\n",
    "    host.set_xticks(range(ys.shape[1]))\n",
    "    host.set_xticklabels(ynames, fontsize=7)\n",
    "    host.tick_params(axis='x', which='major', pad=7)\n",
    "    host.spines['right'].set_visible(False)\n",
    "    host.xaxis.tick_top()\n",
    "    host.set_title(title, fontsize=15)\n",
    "\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors\n",
    "    if category_names is not None:\n",
    "        legend_handles = [None for _ in category_names]\n",
    "    else:\n",
    "        legend_handles = [None for _ in set(category)]\n",
    "    for j in range(N):\n",
    "        # to just draw straight lines between the axes:\n",
    "        # host.plot(range(ys.shape[1]), zs[j,:], c=colors[(category[j] - 1) % len(colors) ])\n",
    "\n",
    "        # create bezier curves\n",
    "        # for each axis, there will a control vertex at the point itself, one at 1/3rd towards the previous and one\n",
    "        #   at one third towards the next axis; the first and last axis have one less control vertex\n",
    "        # x-coordinate of the control vertices: at each integer (for the axes) and two inbetween\n",
    "        # y-coordinate: repeat every point three times, except the first and last only twice\n",
    "        verts = list(zip([x for x in np.linspace(0, len(ys) - 1, len(ys) * 3 - 2, endpoint=True)],\n",
    "                         np.repeat(zs[j, :], 3)[1:-1]))\n",
    "        # for x,y in verts: host.plot(x, y, 'go') # to show the control points of the beziers\n",
    "        codes = [Path.MOVETO] + [Path.CURVE4 for _ in range(len(verts) - 1)]\n",
    "        path = Path(verts, codes)\n",
    "        patch = patches.PathPatch(path, facecolor='none', lw=1, edgecolor=colors[category[j]])\n",
    "        legend_handles[category[j]] = patch\n",
    "        host.add_patch(patch)\n",
    "\n",
    "        if category_names is not None:\n",
    "            host.legend(legend_handles, category_names,\n",
    "                        loc='lower center', bbox_to_anchor=(0.5, -0.18),\n",
    "                        ncol=len(category_names)//2, fancybox=True, shadow=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def barPlot(title, metric_name, directions, results_per_model, colors, savepath=None):\n",
    "\n",
    "    x = np.arange(len(directions))  # the label locations\n",
    "    width = 0.05  # the width of the bars\n",
    "    multiplier = 0\n",
    "    nb_model = len(results_per_model)\n",
    "\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(24, 8))\n",
    "\n",
    "    for i, (model, results) in enumerate(results_per_model.items()):\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, results[\"mean_score\"], width, label=model, yerr=results[\"std_unbias_score\"], align='center', ecolor='black', capsize=2, color = colors[i])\n",
    "        multiplier += 1\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel(f\"{metric_name} score\")\n",
    "    ax.set_ylim(0)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.set_xticks(x + (nb_model//2)*width, directions)\n",
    "    ax.legend(loc='upper center', ncols=len(results_per_model)//2, fancybox=True, shadow=True)\n",
    "    plt.tight_layout()\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model_name(model_name, model_size):\n",
    "    return f\"{model_name}\"+(f\"-{model_size}\" if model_size is not None else \"\")\n",
    "\n",
    "def concatenate_results_parrPlot(directions, models, model_sizes, datasets, reduce_sizes, metrics_names, agg_keys, verbose=False):\n",
    "    \"\"\"\n",
    "    agg_keys should be a list containing keys present in output dictonnary for every metrics desired\n",
    "    for all metrics, can be only [\"mean_score\", \"std_score\", \"std_unbias_score\"] (or less)\n",
    "    \"\"\"\n",
    "    metrics_names2metrics = {\"ROUGE-1\": \"rouge1\",\n",
    "                             \"ROUGE-2\": \"rouge2\",\n",
    "                             \"ROUGE-L\": \"rougeL\",\n",
    "                             \"ROUGE-Lsum\": \"rougeLsum\",\n",
    "                             \"BLEU\": \"bleu\",\n",
    "                             \"SacreBLEU\": \"sacrebleu\",\n",
    "                             \"chrF\": \"chrf\",\n",
    "                             \"chrF++\": \"chrfplusplus\",\n",
    "                             \"COMET\": \"comet\",\n",
    "                             \"BLEURT\": \"bleurt\",\n",
    "                             \"BERTscore\": \"bertscore\",\n",
    "                             \"METEOR\": \"meteor\"}\n",
    "    metrics = [metrics_names2metrics[name] for name in metrics_names] # Want something ordered, don't only take dico.values()\n",
    "    \n",
    "    data = {key: [[] for _ in range(len(metrics))] for key in agg_keys}\n",
    "    \n",
    "    print(\"Extracting and concatenating metrics...\")\n",
    "    for dataset_name, reduce_size in zip(datasets, reduce_sizes):\n",
    "        for model_name, model_size in zip(models, model_sizes):\n",
    "            for direction in directions:\n",
    "                eval_filename = get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "                if verbose:\n",
    "                    print(eval_filename)\n",
    "                with open(eval_filename, \"rb\") as f:\n",
    "                    evaluations = pickle.load(f)\n",
    "                for i, m in enumerate(metrics):\n",
    "                    for key in agg_keys:\n",
    "                        data[key][i].append(evaluations[m][key])\n",
    "    return data\n",
    "\n",
    "def make_parallel_plot(directions,\n",
    "                       models, model_sizes,\n",
    "                       datasets, reduce_sizes,\n",
    "                       metrics_names,\n",
    "                       list_colors_per, colors=None, verbose=False, savepath=None):\n",
    "    # Aggregate eval data\n",
    "    data = concatenate_results_parrPlot(directions, models, model_sizes, datasets, reduce_sizes, metrics_names, agg_keys=[\"mean_score\"], verbose=verbose)\n",
    "    data = data[\"mean_score\"]\n",
    "\n",
    "    # Generate plot categories\n",
    "    ## Precompute categories names\n",
    "    print(f\"Generating categories based {list_colors_per} type ('list_colors_per' param)...\")\n",
    "\n",
    "    dataset_name2real_name = {\"wnt23\": \"WNT23\", \"flores\": \"FLORES+\"}\n",
    "    dataset_name2real_name_and_reduction = {}\n",
    "    for dataset_name, reduce_size in zip(datasets, reduce_sizes):\n",
    "        dataset_name2real_name_and_reduction[dataset_name] = dataset_name2real_name[dataset_name] + f\" - reduct to {reduce_size} samples\"\n",
    "    category_names_data = [dataset_name2real_name_and_reduction[dataset_name] for dataset_name in datasets] if \"dataset\" in list_colors_per else []\n",
    "    category_names_direction = directions if \"direction\" in list_colors_per else []\n",
    "    category_names_models = [get_full_model_name(model_name, model_size) for model_name, model_size in zip(models, model_sizes)] if \"model\" in list_colors_per else []\n",
    "\n",
    "    ## Generate all combinaisons of categories\n",
    "    category_names = []\n",
    "    for cat_data in (category_names_data if len(category_names_data)>0 else [\"\"]):\n",
    "        is_text = len(category_names_data)>0 and (len(category_names_direction)>0 or len(category_names_models)>0)\n",
    "        cat1 = cat_data + (\" - \" if is_text else \"\")\n",
    "        for cat_model in (category_names_models if len(category_names_models)>0 else [\"\"]):\n",
    "            is_text = len(category_names_models)>0 and len(category_names_direction)>0\n",
    "            cat2 = cat1 + cat_model + (\" - \" if is_text else \"\")\n",
    "            for cat_dir in (category_names_direction if len(category_names_direction)>0 else [\"\"]):\n",
    "                cat3 = cat2 + cat_dir\n",
    "                category_names.append(cat3)\n",
    "    elem2cat = {cat_name: i for i, cat_name in enumerate(category_names)}\n",
    "\n",
    "    ## Get category name per element\n",
    "    category = []\n",
    "    for dataset_name in datasets:\n",
    "        for model_name, model_size in zip(models, model_sizes):\n",
    "            for direction in directions:\n",
    "                is_text = len(category_names_data)>0 and (len(category_names_direction)>0 or len(category_names_models)>0)\n",
    "                cat_name = (dataset_name2real_name_and_reduction[dataset_name] if len(category_names_data)>0 else \"\") + (\" - \" if is_text else \"\")\n",
    "                is_text = len(category_names_models)>0 and len(category_names_direction)>0\n",
    "                cat_name = cat_name + (get_full_model_name(model_name, model_size) if len(category_names_models)>0 else \"\") + (\" - \" if is_text else \"\")\n",
    "                cat_name = cat_name + (direction if len(category_names_direction)>0 else \"\")\n",
    "                category.append(elem2cat[cat_name])\n",
    "\n",
    "    if colors is None and len(list_colors_per)==1:\n",
    "        if \"dataset\" in list_colors_per:\n",
    "            colors = plt.cm.Accent.colors\n",
    "        elif \"direction\" in list_colors_per:\n",
    "            colors = plt.cm.tab20.colors\n",
    "        else:\n",
    "            colors = plt.cm.Dark2.colors + plt.cm.tab10.colors[0:7] + plt.cm.tab10.colors[8:]\n",
    "\n",
    "    # Plot\n",
    "    print(\"Plotting in parallel coordinates plot...\")\n",
    "    n_datasets, n_directions, n_models = len(directions), len(models), len(datasets)\n",
    "    parallelCoordinatesPlot(title = f\"Influence of {list_colors_per} on translation performances\",\n",
    "                            N = n_datasets*n_directions*n_models,\n",
    "                            data = data,\n",
    "                            category = category,\n",
    "                            category_names = category_names,\n",
    "                            ynames = metrics_names,\n",
    "                            colors=colors,\n",
    "                            savepath=savepath)\n",
    "\n",
    "def concatenate_results_barPlot(directions, models, model_sizes, dataset_name, reduce_size, metric_name, verbose=False):\n",
    "    \"\"\"\n",
    "    for all metrics, can be only [\"mean_score\", \"std_score\", \"std_unbias_score\"] (or less)\n",
    "    \"\"\"\n",
    "    metrics_names2metrics = {\"ROUGE-1\": \"rouge1\",\n",
    "                             \"ROUGE-2\": \"rouge2\",\n",
    "                             \"ROUGE-L\": \"rougeL\",\n",
    "                             \"ROUGE-Lsum\": \"rougeLsum\",\n",
    "                             \"BLEU\": \"bleu\",\n",
    "                             \"SacreBLEU\": \"sacrebleu\",\n",
    "                             \"chrF\": \"chrf\",\n",
    "                             \"chrF++\": \"chrfplusplus\",\n",
    "                             \"COMET\": \"comet\",\n",
    "                             \"BLEURT\": \"bleurt\",\n",
    "                             \"BERTscore\": \"bertscore\",\n",
    "                             \"METEOR\": \"meteor\"}\n",
    "    metric = metrics_names2metrics[metric_name]\n",
    "    results_per_model = {get_full_model_name(model_name, model_size): {\"mean_score\":[], \"std_unbias_score\":[]} for model_name, model_size in zip(models, model_sizes)}\n",
    "    \n",
    "    print(\"Extracting and concatenating metrics...\")\n",
    "    for model_name, model_size in zip(models, model_sizes):\n",
    "        for direction in directions:\n",
    "            eval_filename = get_eval_filename(direction, dataset_name, model_name, model_size, reduce_size)\n",
    "            if verbose:\n",
    "                print(eval_filename)\n",
    "            with open(eval_filename, \"rb\") as f:\n",
    "                evaluations = pickle.load(f)\n",
    "            results_per_model[get_full_model_name(model_name, model_size)][\"mean_score\"].append(evaluations[metric][\"mean_score\"])\n",
    "            results_per_model[get_full_model_name(model_name, model_size)][\"std_unbias_score\"].append(evaluations[metric][\"std_unbias_score\"])\n",
    "    return results_per_model\n",
    "\n",
    "def make_bar_plot(directions,\n",
    "                    model_names, model_sizes,\n",
    "                    dataset_name, reduce_size,\n",
    "                    metric_names,\n",
    "                    cmap=None,\n",
    "                    savepath = None):\n",
    "    for metric_name in metric_names:\n",
    "        title = f\"{metric_name} translation evaluation on dataset {dataset_name} (mean score with unbiased std)\"\n",
    "        results_per_model = concatenate_results_barPlot(directions, model_names, model_sizes, dataset_name, reduce_size, metric_name, verbose=False)\n",
    "        cmap = \"Spectral\" if cmap is None else cmap\n",
    "        cmap_perso = ListedColormap(sns.color_palette(cmap, len(results_per_model)).as_hex())\n",
    "        barPlot(title,\n",
    "                metric_name,\n",
    "                directions,\n",
    "                results_per_model,\n",
    "                colors = cmap_perso.colors,\n",
    "                savepath = (savepath+f\"_{metric_name}\" if savepath is not None else None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICL pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tgt_icl_fn_CausalModel(number_examples):\n",
    "    \"\"\"Works for ALMA, OPT-instruct, BLOOMz, and any GPT model non instruct\"\"\"\n",
    "    def get_input_targets_icl_CausalModel(dataset, source_lang, target_lang):\n",
    "        language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "        source_lang_name = language_name[source_lang]\n",
    "        target_lang_name = language_name[target_lang]\n",
    "        # Use base formulation \"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "        sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "\n",
    "        inputs = []\n",
    "        offset_seed = 0\n",
    "        print(\"Generating prompts for In-Context learning...\")\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            np.random.seed(i + offset_seed)\n",
    "            idx = np.arange(len(dataset))\n",
    "            idx = np.random.choice(idx, number_examples)\n",
    "            while i in idx: # Make sure the translation to do is not in the examples\n",
    "                offset_seed += 1\n",
    "                np.random.seed(i + offset_seed)\n",
    "                idx = np.arange(len(dataset))\n",
    "                idx = np.random.choice(idx, number_examples)\n",
    "            examples = [dataset[f\"{source_lang}-{target_lang}\"][n] for n in idx]\n",
    "            inp = f\"Here are examples of translations from {source_lang_name} to {target_lang_name}:\"\n",
    "            for n in range(number_examples):\n",
    "                example_source, example_target = examples[n][source_lang], examples[n][target_lang]\n",
    "                inp += f\"[START]\\n{source_lang_name}: {example_source} \\n{target_lang_name}: {example_target}\\n[END]\"\n",
    "            inp += f\"\\n Using the examples, translate from {source_lang_name} to {target_lang_name}:\"\n",
    "            input_source = dataset[f\"{source_lang}-{target_lang}\"][i][source_lang]\n",
    "            inp += f\"[START]\\n{source_lang_name}: {input_source} \\n{target_lang_name}:\"\n",
    "            inputs.append(inp)\n",
    "\n",
    "        targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "        return sources, inputs, targets\n",
    "    return get_input_targets_icl_CausalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful, the code runs for ALMA, but 16GB is not enough to use 7B models quantized in 8 bits with one example...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "dataset_names = [\"wnt23\"]\n",
    "\n",
    "model_names = [\"opt-instruct\"]\n",
    "model_sizes = [None]\n",
    "\n",
    "batch_size = 1\n",
    "reduce_size = 4\n",
    "\n",
    "number_examples = 1\n",
    "\n",
    "generate_translation_several_datasets(directions, dataset_names, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                    load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                    get_input_targets_fn = get_input_tgt_icl_fn_CausalModel(number_examples),\n",
    "                                    tslt_fn = translate_batched_OPT,\n",
    "                                    translation_folder = f\"evaluationsICL_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Instruct Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tgt_icl_fn_Instruct(number_examples):\n",
    "    def get_input_targets_icl_Instruct(dataset, source_lang, target_lang):\n",
    "        \"\"\"\n",
    "        Work at least for Qwen2.5 and Llama3\n",
    "        \"\"\"\n",
    "        language_name = {\"en\": \"English\", \"de\": \"German\", \"ru\": \"Russian\", \"is\": \"Islandic\", \"zh\": \"Chinese\", \"cs\": \"Czech\"}\n",
    "        source_lang_name = language_name[source_lang]\n",
    "        target_lang_name = language_name[target_lang]\n",
    "        sources = [example[source_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "\n",
    "        inputs = []\n",
    "        offset_seed = 0\n",
    "        print(\"Generating prompts for In-Context learning...\")\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            np.random.seed(i + offset_seed)\n",
    "            idx = np.arange(len(dataset))\n",
    "            idx = np.random.choice(idx, number_examples)\n",
    "            while i in idx: # Make sure the translation to do is not in the examples\n",
    "                offset_seed += 1\n",
    "                np.random.seed(i + offset_seed)\n",
    "                idx = np.arange(len(dataset))\n",
    "                idx = np.random.choice(idx, number_examples)\n",
    "            examples = [dataset[f\"{source_lang}-{target_lang}\"][n] for n in idx]\n",
    "            inp = f\"Here are examples of translations from {source_lang_name} to {target_lang_name}:\"\n",
    "            for n in range(number_examples):\n",
    "                example_source, example_target = examples[n][source_lang], examples[n][target_lang]\n",
    "                inp += f\"\\n[EXAMPLE {n+1}]\\n{source_lang_name}: {example_source} \\n{target_lang_name}: {example_target}\"\n",
    "            inp += f\"\\n Using the examples, translate from {source_lang_name} to {target_lang_name}:\"\n",
    "            input_source = dataset[f\"{source_lang}-{target_lang}\"][i][source_lang]\n",
    "            inp += f\"[TASK]\\n{source_lang_name}: {input_source} \\n{target_lang_name}:\"\n",
    "            inputs.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a translator, you output only the translation in the desired language.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{inp}\"}])\n",
    "\n",
    "        targets = [example[target_lang] for example in dataset[f\"{source_lang}-{target_lang}\"]]\n",
    "        return sources, inputs, targets\n",
    "    return get_input_targets_icl_Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"en-de\", \"de-en\",\n",
    "              \"en-cs\", \"cs-en\",\n",
    "              \"en-is\", \"is-en\",\n",
    "              \"en-zh\", \"zh-en\",\n",
    "              \"en-ru\", \"ru-en\"]\n",
    "dataset_names = [\"wnt23\"]\n",
    "\n",
    "model_names = [\"llama3\"]\n",
    "model_sizes = [\"3B\"]\n",
    "\n",
    "batch_size = 1\n",
    "reduce_size = 50\n",
    "\n",
    "for number_examples in [1, 2, 3, 4]: #More than 4 is OOM\n",
    "    generate_translation_several_datasets(directions, dataset_names, model_names, model_sizes, batch_size, reduce_size,\n",
    "                                        load_model_and_tokenizer_fn = load_model_benchmark,\n",
    "                                        get_input_targets_fn = get_input_tgt_icl_fn_Instruct(number_examples),\n",
    "                                        tslt_fn = translate_batched_Llama3,\n",
    "                                        translation_folder = f\"evaluationsICL_{number_examples}examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for number_examples in [1, 2, 3, 4]:\n",
    "    with open(f\"./generated_translations/evaluationsICL_{number_examples}examples/wnt23_llama3-3B_en-de_red-50.pkl\", \"rb\") as f:\n",
    "        translations = pickle.load(f)\n",
    "    print(\"First translation:\", translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP environment",
   "language": "python",
   "name": "snlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
